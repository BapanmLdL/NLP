{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0pqiCQbR5p0G"
   },
   "source": [
    "#Pdf to Text (as of now no need)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pdfminer\n",
      "  Downloading pdfminer-20191125.tar.gz (4.2 MB)\n",
      "Collecting pycryptodome\n",
      "  Downloading pycryptodome-3.11.0-cp35-abi3-win_amd64.whl (1.8 MB)\n",
      "Building wheels for collected packages: pdfminer\n",
      "  Building wheel for pdfminer (setup.py): started\n",
      "  Building wheel for pdfminer (setup.py): finished with status 'done'\n",
      "  Created wheel for pdfminer: filename=pdfminer-20191125-py3-none-any.whl size=6140083 sha256=1359c8a35743cb91580ce4567d641433dc5bf676f1757eb86f0e34da7da55779\n",
      "  Stored in directory: c:\\users\\bapan\\appdata\\local\\pip\\cache\\wheels\\1c\\28\\7d\\f390b82bb0307deb63ff27a1474fd308ec68ee028cb9ab6283\n",
      "Successfully built pdfminer\n",
      "Installing collected packages: pycryptodome, pdfminer\n",
      "Successfully installed pdfminer-20191125 pycryptodome-3.11.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pdfminer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 938,
     "status": "ok",
     "timestamp": 1605845930899,
     "user": {
      "displayName": "Vishnu Murali",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi9HFkIyK2pSuiQeQKuJ_g129BxLvMVLl9xboj8=s64",
      "userId": "16837812932775871984"
     },
     "user_tz": -330
    },
    "id": "eWLK2m4b3LgH"
   },
   "outputs": [],
   "source": [
    "from pdfminer.pdfparser import PDFParser\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.converter import PDFPageAggregator\n",
    "from pdfminer.layout import LAParams, LTTextBox, LTTextLine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 34006,
     "status": "ok",
     "timestamp": 1605845925020,
     "user": {
      "displayName": "Vishnu Murali",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi9HFkIyK2pSuiQeQKuJ_g129BxLvMVLl9xboj8=s64",
      "userId": "16837812932775871984"
     },
     "user_tz": -330
    },
    "id": "tOJ3qU0g34vB",
    "outputId": "703ed641-0073-46e1-9a11-c46ae0a9326d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pdfminer\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/a3/155c5cde5f9c0b1069043b2946a93f54a41fd72cc19c6c100f6f2f5bdc15/pdfminer-20191125.tar.gz (4.2MB)\n",
      "\u001b[K     |████████████████████████████████| 4.2MB 3.7MB/s \n",
      "\u001b[?25hCollecting pycryptodome\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2b/6f/7e38d7c97fbbc3987539c804282c33f56b6b07381bf2390deead696440c5/pycryptodome-3.9.9-cp36-cp36m-manylinux1_x86_64.whl (13.7MB)\n",
      "\u001b[K     |████████████████████████████████| 13.7MB 321kB/s \n",
      "\u001b[?25hBuilding wheels for collected packages: pdfminer\n",
      "  Building wheel for pdfminer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for pdfminer: filename=pdfminer-20191125-cp36-none-any.whl size=6140070 sha256=65d00eea884eea5e321ecd40fdf2b77bba1a08f5ddb1fed526325224fbda51b8\n",
      "  Stored in directory: /root/.cache/pip/wheels/e1/00/af/720a55d74ba3615bb4709a3ded6dd71dc5370a586a0ff6f326\n",
      "Successfully built pdfminer\n",
      "Installing collected packages: pycryptodome, pdfminer\n",
      "Successfully installed pdfminer-20191125 pycryptodome-3.9.9\n"
     ]
    }
   ],
   "source": [
    "!pip install pdfminer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 1026,
     "status": "ok",
     "timestamp": 1605845940898,
     "user": {
      "displayName": "Vishnu Murali",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi9HFkIyK2pSuiQeQKuJ_g129BxLvMVLl9xboj8=s64",
      "userId": "16837812932775871984"
     },
     "user_tz": -330
    },
    "id": "1ncw_GaV3LgI"
   },
   "outputs": [],
   "source": [
    "from pdfminer.pdfdocument import PDFDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 1050,
     "status": "ok",
     "timestamp": 1605845943588,
     "user": {
      "displayName": "Vishnu Murali",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi9HFkIyK2pSuiQeQKuJ_g129BxLvMVLl9xboj8=s64",
      "userId": "16837812932775871984"
     },
     "user_tz": -330
    },
    "id": "gG8XsusG3LgI"
   },
   "outputs": [],
   "source": [
    "extracted_text = \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 951,
     "status": "ok",
     "timestamp": 1605846030841,
     "user": {
      "displayName": "Vishnu Murali",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi9HFkIyK2pSuiQeQKuJ_g129BxLvMVLl9xboj8=s64",
      "userId": "16837812932775871984"
     },
     "user_tz": -330
    },
    "id": "tw4eltJe3LgI"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'doc.pdf'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-70f07b6c6025>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mfp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'doc.pdf'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'doc.pdf'"
     ]
    }
   ],
   "source": [
    "fp = open('doc.pdf', 'rb')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jUm7jLRJ5wmY"
   },
   "source": [
    "#Code starts here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 2617,
     "status": "ok",
     "timestamp": 1605846220360,
     "user": {
      "displayName": "Vishnu Murali",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi9HFkIyK2pSuiQeQKuJ_g129BxLvMVLl9xboj8=s64",
      "userId": "16837812932775871984"
     },
     "user_tz": -330
    },
    "id": "pQfW3cIn3LgJ"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'textblob'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-a1c9ccb849aa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtextblob\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTextBlob\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'textblob'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from textblob import TextBlob\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "matplotlib.rcParams['figure.figsize'] = (10.0, 6.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 1591,
     "status": "ok",
     "timestamp": 1605846224816,
     "user": {
      "displayName": "Vishnu Murali",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi9HFkIyK2pSuiQeQKuJ_g129BxLvMVLl9xboj8=s64",
      "userId": "16837812932775871984"
     },
     "user_tz": -330
    },
    "id": "xblwtO3K3LgJ"
   },
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 925,
     "status": "ok",
     "timestamp": 1605846320236,
     "user": {
      "displayName": "Vishnu Murali",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi9HFkIyK2pSuiQeQKuJ_g129BxLvMVLl9xboj8=s64",
      "userId": "16837812932775871984"
     },
     "user_tz": -330
    },
    "id": "lNP_WIpK3LgJ"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'en_core_web_sm'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-7c52687f2687>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlang\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0men\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mEnglish\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0men_core_web_sm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'en_core_web_sm'"
     ]
    }
   ],
   "source": [
    "from spacy.lang.en import English\n",
    "import en_core_web_sm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 995,
     "status": "ok",
     "timestamp": 1605846302701,
     "user": {
      "displayName": "Vishnu Murali",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi9HFkIyK2pSuiQeQKuJ_g129BxLvMVLl9xboj8=s64",
      "userId": "16837812932775871984"
     },
     "user_tz": -330
    },
    "id": "VweHWj-U42tm"
   },
   "outputs": [],
   "source": [
    "extracted_text='Since 2006, deep learning (DL) has become a rapidly growing research direction, redefining.After the brief introduction of deep learning.'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 1047,
     "status": "ok",
     "timestamp": 1605846294909,
     "user": {
      "displayName": "Vishnu Murali",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi9HFkIyK2pSuiQeQKuJ_g129BxLvMVLl9xboj8=s64",
      "userId": "16837812932775871984"
     },
     "user_tz": -330
    },
    "id": "984KKobC3LgJ"
   },
   "outputs": [],
   "source": [
    "text = extracted_text.split('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 142
    },
    "executionInfo": {
     "elapsed": 1193,
     "status": "ok",
     "timestamp": 1605846400923,
     "user": {
      "displayName": "Vishnu Murali",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi9HFkIyK2pSuiQeQKuJ_g129BxLvMVLl9xboj8=s64",
      "userId": "16837812932775871984"
     },
     "user_tz": -330
    },
    "id": "p9rm0xWw3LgJ",
    "outputId": "82070967-56f5-4f8d-bf56-a31e77c68b3f",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Since 2006, deep learning (DL) has become a ra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>After the brief introduction of deep learning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  Since 2006, deep learning (DL) has become a ra...\n",
       "1      After the brief introduction of deep learning\n",
       "2                                                   "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(text, columns = ['text'])\n",
    "get_ipython().config.get('IPKernelApp', {})['parent_appname'] = \"\"\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 910,
     "status": "ok",
     "timestamp": 1605846260003,
     "user": {
      "displayName": "Vishnu Murali",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi9HFkIyK2pSuiQeQKuJ_g129BxLvMVLl9xboj8=s64",
      "userId": "16837812932775871984"
     },
     "user_tz": -330
    },
    "id": "L92k88KX3LgJ",
    "outputId": "a04c30dc-63cc-4d2a-a418-0b8055f8c098"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 80
    },
    "executionInfo": {
     "elapsed": 1080,
     "status": "ok",
     "timestamp": 1605846262918,
     "user": {
      "displayName": "Vishnu Murali",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi9HFkIyK2pSuiQeQKuJ_g129BxLvMVLl9xboj8=s64",
      "userId": "16837812932775871984"
     },
     "user_tz": -330
    },
    "id": "eCWS13pp3LgJ",
    "outputId": "5c39ceb0-902d-4109-aabc-c92545c6eadb"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Since 2006, deep learning (DL) has become a ra...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  Since 2006, deep learning (DL) has become a ra..."
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_ipython().config.get('IPKernelApp', {})['parent_appname'] = \"\"\n",
    "\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gp9W7k2i3LgJ"
   },
   "source": [
    "# Top unigrams before removing stop words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aRpx_dE23LgJ"
   },
   "outputs": [],
   "source": [
    "def get_top_n_words(corpus, n=None):\n",
    "    vec = CountVectorizer().fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq[:n]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ktbQ1sPK3LgJ"
   },
   "outputs": [],
   "source": [
    "common_words = get_top_n_words(df['text'], 20)\n",
    "for word, freq in common_words:\n",
    "    print(word, freq)\n",
    "df1 = pd.DataFrame(common_words, columns = ['ReviewText' , 'count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eXIRTHrV3LgJ"
   },
   "outputs": [],
   "source": [
    "get_ipython().config.get('IPKernelApp', {})['parent_appname'] = \"\"\n",
    "\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xFGBrJsb3LgJ"
   },
   "outputs": [],
   "source": [
    "df1.groupby('ReviewText').sum()['count'].sort_values(ascending=False).iplot(\n",
    "     kind='bar', yTitle='Count', linecolor='black', title='Top 20 words in review before removing stop words')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PqqzVFE83LgJ"
   },
   "source": [
    "# Top unigrams after removing stop words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y7uimsr53LgJ"
   },
   "outputs": [],
   "source": [
    "def get_top_n_words(corpus, n=None):\n",
    "    vec = CountVectorizer(stop_words = 'english').fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq[:n]\n",
    "common_words = get_top_n_words(df['text'], 20)\n",
    "for word, freq in common_words:\n",
    "    print(word, freq)\n",
    "df2 = pd.DataFrame(common_words, columns = ['ReviewText' , 'count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UXs8riwr3LgJ"
   },
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({\"Keywords\":df2['ReviewText'], \"Count\":df2['count']})\n",
    "submission.to_csv(\"keywords.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "APBxUTLX3LgJ"
   },
   "outputs": [],
   "source": [
    "df2.groupby('ReviewText').sum()['count'].sort_values(ascending=False).iplot(\n",
    "kind='bar', yTitle='Count', linecolor='black', title='Top 20 words in review after removing stop words')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DeRhuM6k3LgJ"
   },
   "source": [
    "# Top bigrams after removing stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zAVmQPl03LgJ"
   },
   "outputs": [],
   "source": [
    "def get_top_n_bigram(corpus, n=None):\n",
    "    vec = CountVectorizer(ngram_range=(2, 2), stop_words='english').fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq[:n]\n",
    "common_words = get_top_n_bigram(df['text'], 20)\n",
    "for word, freq in common_words:\n",
    "    print(word, freq)\n",
    "df4 = pd.DataFrame(common_words, columns = ['ReviewText' , 'count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nHm7hQ1s3LgJ",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df4.groupby('ReviewText').sum()['count'].sort_values(ascending=False).iplot(\n",
    "    kind='bar', yTitle='Count', linecolor='black', title='Top 20 bigrams in review after removing stop words')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wmhMIyCc3LgJ"
   },
   "source": [
    "# Top trigrams after removing stop words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i6-w7lQB3LgJ"
   },
   "outputs": [],
   "source": [
    "def get_top_n_trigram(corpus, n=None):\n",
    "    vec = CountVectorizer(ngram_range=(3, 3), stop_words='english').fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq[:n]\n",
    "common_words = get_top_n_trigram(df['text'], 20)\n",
    "for word, freq in common_words:\n",
    "    print(word, freq)\n",
    "df6 = pd.DataFrame(common_words, columns = ['ReviewText' , 'count'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TfaDvpp43LgJ"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UURMRlXQ3LgJ"
   },
   "outputs": [],
   "source": [
    "df6.groupby('ReviewText').sum()['count'].sort_values(ascending=False).iplot(\n",
    "    kind='bar', yTitle='Count', linecolor='black', title='Top 20 trigrams in review after removing stop words')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "chK_P-Qb3LgJ"
   },
   "source": [
    "# Top 20 part-of-speech tagging of review corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cvVVWmxj3LgJ"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CIk-7PLY3LgJ"
   },
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IwKA6IfR3LgJ"
   },
   "outputs": [],
   "source": [
    "blob = TextBlob(str(df['text']))\n",
    "pos_df = pd.DataFrame(blob.tags, columns = ['word' , 'pos'])\n",
    "pos_df = pos_df.pos.value_counts()[:20]\n",
    "pos_df.iplot(\n",
    "    kind='bar',\n",
    "    xTitle='POS',\n",
    "    yTitle='count', \n",
    "    title='Top 20 Part-of-speech tagging for review corpus')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JpbKstrg3LgJ"
   },
   "source": [
    "# Topic Modeling with LSA (Latent Semantic Analysis)\n",
    "#https://medium.com/nanonets/topic-modeling-with-lsa-psla-lda-and-lda2vec-555ff65b0b05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_ffjN3jp3LgJ"
   },
   "outputs": [],
   "source": [
    "reindexed_data = df['text']\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', use_idf=True, smooth_idf=True)\n",
    "reindexed_data = reindexed_data.values\n",
    "document_term_matrix = tfidf_vectorizer.fit_transform(reindexed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "06WT_d_B3LgJ"
   },
   "outputs": [],
   "source": [
    "n_topics = 6\n",
    "lsa_model = TruncatedSVD(n_components=n_topics)\n",
    "lsa_topic_matrix = lsa_model.fit_transform(document_term_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YkN2at8H3LgJ"
   },
   "outputs": [],
   "source": [
    "def get_keys(topic_matrix):\n",
    "    '''\n",
    "    returns an integer list of predicted topic \n",
    "    categories for a given topic matrix\n",
    "    '''\n",
    "    keys = topic_matrix.argmax(axis=1).tolist()\n",
    "    return keys\n",
    "\n",
    "def keys_to_counts(keys):\n",
    "    '''\n",
    "    returns a tuple of topic categories and their \n",
    "    accompanying magnitudes for a given list of keys\n",
    "    '''\n",
    "    count_pairs = Counter(keys).items()\n",
    "    categories = [pair[0] for pair in count_pairs]\n",
    "    counts = [pair[1] for pair in count_pairs]\n",
    "    return (categories, counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pw2Rk3CO3LgJ"
   },
   "outputs": [],
   "source": [
    "lsa_keys = get_keys(lsa_topic_matrix)\n",
    "lsa_categories, lsa_counts = keys_to_counts(lsa_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P42vOTU13LgJ"
   },
   "outputs": [],
   "source": [
    "def get_top_n_words(n, keys, document_term_matrix, tfidf_vectorizer):\n",
    "    '''\n",
    "    returns a list of n_topic strings, where each string contains the n most common \n",
    "    words in a predicted category, in order\n",
    "    '''\n",
    "    top_word_indices = []\n",
    "    for topic in range(n_topics):\n",
    "        temp_vector_sum = 0\n",
    "        for i in range(len(keys)):\n",
    "            if keys[i] == topic:\n",
    "                temp_vector_sum += document_term_matrix[i]\n",
    "        temp_vector_sum = temp_vector_sum.toarray()\n",
    "        top_n_word_indices = np.flip(np.argsort(temp_vector_sum)[0][-n:],0)\n",
    "        top_word_indices.append(top_n_word_indices)   \n",
    "    top_words = []\n",
    "    for topic in top_word_indices:\n",
    "        topic_words = []\n",
    "        for index in topic:\n",
    "            temp_word_vector = np.zeros((1,document_term_matrix.shape[1]))\n",
    "            temp_word_vector[:,index] = 1\n",
    "            the_word = tfidf_vectorizer.inverse_transform(temp_word_vector)[0][0]\n",
    "            topic_words.append(the_word.encode('ascii').decode('utf-8'))\n",
    "        top_words.append(\" \".join(topic_words))         \n",
    "    return top_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uo9Jg6RH3LgJ"
   },
   "outputs": [],
   "source": [
    "top_n_words_lsa = get_top_n_words(3, lsa_keys, document_term_matrix, tfidf_vectorizer)\n",
    "\n",
    "for i in range(len(top_n_words_lsa)):\n",
    "    print(\"Topic {}: \".format(i+1), top_n_words_lsa[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2JThRrJD3LgJ"
   },
   "outputs": [],
   "source": [
    "\n",
    "top_3_words = get_top_n_words(3, lsa_keys, document_term_matrix, tfidf_vectorizer)\n",
    "labels = ['Topic {}: \\n'.format(i) + top_3_words[i] for i in lsa_categories]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16,8))\n",
    "ax.bar(lsa_categories, lsa_counts);\n",
    "ax.set_xticks(lsa_categories);\n",
    "ax.set_xticklabels(labels);\n",
    "ax.set_ylabel('Number of review text');\n",
    "ax.set_title('LSA topic counts');\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vShYFJpP3LgJ"
   },
   "source": [
    "# Text Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bi02dP8j3LgJ"
   },
   "outputs": [],
   "source": [
    "from gensim.summarization.summarizer import summarize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bv9Dq4xd3LgJ"
   },
   "outputs": [],
   "source": [
    "extracted_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R15j6WTq3LgJ"
   },
   "outputs": [],
   "source": [
    "print(summarize(extracted_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wwJOWHb_3LgJ"
   },
   "source": [
    "## Word cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sIEg8hkl3LgJ"
   },
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HfmnYE-x3LgJ"
   },
   "outputs": [],
   "source": [
    "comment_words = ' '\n",
    "stopwords = set(STOPWORDS) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rj3x_kZa3LgJ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Text analytics_Basics_ for single text file.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
