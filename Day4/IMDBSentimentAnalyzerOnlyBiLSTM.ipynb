{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This demonstrates the following:\n",
    "How to load imdb data\n",
    "How load converts words into integer indexes\n",
    "How words are stored in highest occuring freq to lowest occurring\n",
    "How to take top 5000 or 10000 vocbualry size\n",
    "idiosyncracy of imdb.get_word_index() \n",
    "Re-convert the index to take into account padding, start, out-of-vocabulary\n",
    "What is an embedding layer\n",
    "Create a ConvNet model\n",
    "Create a RNN model\n",
    "Create a LSTM model\n",
    "DO the sentiment analyzer\n",
    "Predict sample sentences\n",
    "Then this takes into advanced featues of Keras such as callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.datasets import imdb\n",
    "from keras.layers import Dense,  Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMDB dataset  25000 training samples, 25000 test samples\n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = 10000\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words = vocabulary_size)\n",
    "print('IMDB dataset  {} training samples, {} test samples'.format(len(X_train), len(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Features: review in number sequence After padding---\n",
      "[1, 6740, 365, 1234, 5, 1156, 354, 11, 14, 5327, 6638, 7, 1016, 2, 5940, 356, 44, 4, 1349, 500, 746, 5, 200, 4, 4132, 11, 2, 9363, 1117, 1831, 7485, 5, 4831, 26, 6, 2, 4183, 17, 369, 37, 215, 1345, 143, 2, 5, 1838, 8, 1974, 15, 36, 119, 257, 85, 52, 486, 9, 6, 2, 8564, 63, 271, 6, 196, 96, 949, 4121, 4, 2, 7, 4, 2212, 2436, 819, 63, 47, 77, 7175, 180, 6, 227, 11, 94, 2494, 2, 13, 423, 4, 168, 7, 4, 22, 5, 89, 665, 71, 270, 56, 5, 13, 197, 12, 161, 5390, 99, 76, 23, 2, 7, 419, 665, 40, 91, 85, 108, 7, 4, 2084, 5, 4773, 81, 55, 52, 1901]\n",
      "Sentiment 1 = positive, 0 = negative\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print('---Features: review in number sequence After padding---')\n",
    "print(X_train[6])\n",
    "print('Sentiment 1 = positive, 0 = negative')\n",
    "print(y_train[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88584 the 1\n"
     ]
    }
   ],
   "source": [
    "w2id = imdb.get_word_index() \n",
    "id2word = {i: word  for word, i in w2id.items()}\n",
    "print (len(w2id), id2word[1], w2id[\"the\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the boiled full involving to impressive boring this as murdering naschy br villain and suggestion need has of costumes b message to may of props this and concentrates concept issue skeptical to god's he is and unfolds movie women like isn't surely i'm and to toward in here's for from did having because very quality it is and starship really book is both too worked carl of and br of reviewer closer figure really there will originals things is far this make mistakes and was couldn't of few br of you to don't female than place she to was between that nothing dose movies get are and br yes female just its because many br of overly to descent people time very bland \n",
      "And sentiment is  1\n"
     ]
    }
   ],
   "source": [
    "#gibberish data?\n",
    "actual_data = []\n",
    "for i in range (len(X_train[6])):\n",
    "    ind = X_train[6][i]\n",
    "    if(ind != 0):\n",
    "        actual_data.append(id2word[ind])\n",
    "actual_data = \" \".join (actual_data )\n",
    "print (actual_data, \"\\nAnd sentiment is \", y_train[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'> 88587\n",
      "__START__ __UNK__ the and a voorhees'\n",
      "__START__ lavish production values and solid performances in this straightforward adaption of jane __UNK__ satirical classic about the marriage game within and between the classes in __UNK__ 18th century england northam and paltrow are a __UNK__ mixture as friends who must pass through __UNK__ and lies to discover that they love each other good humor is a __UNK__ virtue which goes a long way towards explaining the __UNK__ of the aged source material which has been toned down a bit in its harsh __UNK__ i liked the look of the film and how shots were set up and i thought it didn't rely too much on __UNK__ of head shots like most other films of the 80s and 90s do very good results \n",
      "And sentiment is  1\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "word2id ={w: i+3 for w, i in w2id.items()}\n",
    "word2id[\"__PADDING__\"] = 0\n",
    "word2id[\"__START__\"] = 1\n",
    "word2id[\"__UNK__\"] = 2\n",
    "\n",
    "#This returns the index of the words from 1 to n with 1 being the most frequently occuring word, \n",
    "\n",
    "# and n the least frequently occuring word\n",
    "\n",
    "print (type(word2id), len (word2id))\n",
    "\n",
    "id2word = {i: word  for word, i in word2id.items()}\n",
    "print(id2word[1], id2word[2], #id2word[3],#\n",
    "      id2word[4], id2word[5], id2word[6], id2word[88586])\n",
    "actual_data = []\n",
    "for i in range (len(X_train[6])):\n",
    "    ind = X_train[6][i]\n",
    "    if(ind != 0):\n",
    "        actual_data.append(id2word[ind])\n",
    "actual_data = \" \".join (actual_data )\n",
    "print (actual_data, \"\\nAnd sentiment is \", y_train[6])\n",
    "print (word2id[\"the\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum review length: 2697\n"
     ]
    }
   ],
   "source": [
    "print('Maximum review length: {}'.format(\n",
    "len((max((X_train + X_test), key=len)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "max_words = 500\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_words)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_words)\n",
    "modelLSTM=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now create a simple RNN model and lets see the accuracy\n",
    "from keras.layers import SimpleRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.4\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 500)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 500, 50)      500000      input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape_3 (Reshape)             (None, 500, 50)      0           embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_5 (LSTM)                   (None, 150)          120600      reshape_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_6 (LSTM)                   (None, 150)          120600      reshape_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 300)          0           lstm_5[0][0]                     \n",
      "                                                                 lstm_6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 1)            301         concatenate_3[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 741,501\n",
      "Trainable params: 741,501\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#LEts now create a bi-directional LSTM model\n",
    "print (keras.__version__)\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout, Reshape\n",
    "from keras.layers.merge import concatenate\n",
    "from keras import Input\n",
    "from keras.models import Model\n",
    "\n",
    "embedding_size=50\n",
    "deep_inputs = Input(shape=(max_words,))\n",
    "x = Embedding(vocabulary_size, embedding_size, input_length=max_words)(deep_inputs)\n",
    "x = Reshape((max_words,embedding_size) )(x)\n",
    "\n",
    "\n",
    "modelLSTM_Left = LSTM(150, dropout=0.2)(x)\n",
    "\n",
    "modelLSTM_Right = LSTM(150, dropout=0.2,go_backwards = True)(x)\n",
    "modelMerged = concatenate([modelLSTM_Left, modelLSTM_Right])\n",
    "out = Dense(1, activation='sigmoid')(modelMerged)\n",
    "modelBi = Model(inputs = deep_inputs, outputs = out)\n",
    "modelBi.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelBi.compile(loss='binary_crossentropy', \n",
    "             optimizer='adam', \n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "num_epochs = 5\n",
    "X_valid, y_valid = X_train[:batch_size], y_train[:batch_size]\n",
    "X_train2, y_train2 = X_train[batch_size:], y_train[batch_size:]\n",
    "callback_list_BI_RNN = [#early1\n",
    "                keras.callbacks.ModelCheckpoint(filepath=\"my_modBiLSTM_BestValAcc.h5\", monitor=\"val_acc\",\n",
    "                                               save_best_only=True),\n",
    "                #keras.callbacks.TerminateOnNaN()\n",
    "                ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now = 2020-09-29 16:03:12.110119\n",
      "Current Date and time = 29/09/2020 16:03:12\n",
      "Train on 24744 samples, validate on 256 samples\n",
      "Epoch 1/5\n",
      "24744/24744 [==============================] - 3042s 123ms/step - loss: 0.6587 - acc: 0.6599 - val_loss: 0.5854 - val_acc: 0.6602\n",
      "Epoch 2/5\n",
      "24744/24744 [==============================] - 4404s 178ms/step - loss: 0.3881 - acc: 0.8344 - val_loss: 0.2681 - val_acc: 0.9062\n",
      "Epoch 3/5\n",
      "24744/24744 [==============================] - 4938s 200ms/step - loss: 0.2780 - acc: 0.8914 - val_loss: 0.2292 - val_acc: 0.9336\n",
      "Epoch 4/5\n",
      "24744/24744 [==============================] - 4600s 186ms/step - loss: 0.2030 - acc: 0.9253 - val_loss: 0.2214 - val_acc: 0.9102\n",
      "Epoch 5/5\n",
      "24744/24744 [==============================] - 5811s 235ms/step - loss: 0.1605 - acc: 0.9446 - val_loss: 0.2387 - val_acc: 0.8945\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'modelRNN' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-0d33d51b4742>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m modelBi.fit(X_train2, y_train2, validation_data=(X_valid, y_valid), batch_size=batch_size, epochs=num_epochs,\n\u001b[0;32m     13\u001b[0m             callbacks=callback_list_BI_RNN)\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0mmodelRNN\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"my_modBiLSTMRNN_Latest\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'modelRNN' is not defined"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# datetime object containing current date and time\n",
    "now = datetime.now()\n",
    " \n",
    "print(\"now =\", now)\n",
    "\n",
    "# dd/mm/YY H:M:S\n",
    "dt_string = now.strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "print(\"Current Date and time =\", dt_string)\n",
    "\n",
    "modelBi.fit(X_train2, y_train2, validation_data=(X_valid, y_valid), batch_size=batch_size, epochs=num_epochs,\n",
    "            callbacks=callback_list_BI_RNN)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelBi.load_weights(\"my_modBiLSTMRNN_Latest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we will load the bestBiModel also\n",
    "deep_inputs = Input(shape=(max_words,))\n",
    "x = Embedding(vocabulary_size, embedding_size, input_length=max_words)(deep_inputs)\n",
    "x = Reshape((max_words,embedding_size) )(x)\n",
    "\n",
    "\n",
    "modelLSTM_Left = LSTM(150, dropout=0.2)(x)\n",
    "\n",
    "modelLSTM_Right = LSTM(150, dropout=0.2,go_backwards = True)(x)\n",
    "modelMerged = concatenate([modelLSTM_Left, modelLSTM_Right])\n",
    "out = Dense(1, activation='sigmoid')(modelMerged)\n",
    "modelBestBi = Model(inputs = deep_inputs, outputs = out)\n",
    "modelBestBi.load_weights(\"my_modBiLSTM_BestValAcc.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now going to predict these reviews\n"
     ]
    }
   ],
   "source": [
    "reviewList = [\"the movie was boring\",              \n",
    "              \"the movie was a total waste of my time.  wish had not seen it\",\n",
    "              \"that the service lasted only 2 minutes was the only saving grace on the \\\n",
    "              otherwise unknown issue. I will never stop doubting their credentials\",\n",
    "              \"am unhappy at receiving this, You are so late, \\\n",
    "              that would have retired by the time got this.   am not going to allow ,  \\\n",
    "              which by the way is a lot at this late date.\\\n",
    "             frustrated at your courtesy\",\n",
    "            \"You may have reasons to feel pleased about yourself. You would think that  am \\\n",
    "            happy at \\\n",
    "            receiving this. Unfortunately you are wrong. Very wrong. As Wrong as sky! \\\n",
    "              not going to allow the amount,  \\\n",
    "              which by the way is a lot  .\\\n",
    "             Don't be so  happy.Am not and thats what matters! \",\n",
    "              \n",
    "             ]\n",
    "print (\"Now going to predict these reviews\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "print (word2id[\"the\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "review= the movie was boring\n",
      "word= the id= 4\n",
      "word= movie id= 20\n",
      "word= was id= 16\n",
      "word= boring id= 357\n",
      "Prediction Probability for  \" the movie was boring \" Bi Directional BEST acc RNN-LSTM  =  0.012319153 Sentiment= Negative \n",
      "\n",
      "Prediction Probability for  \" the movie was boring \" Bi Directional RNN-LSTM  =  0.49965024 Sentiment= Negative \n",
      "\n",
      "review= the movie was a total waste of my time.  wish had not seen it\n",
      "word= the id= 4\n",
      "word= movie id= 20\n",
      "word= was id= 16\n",
      "word= a id= 6\n",
      "word= total id= 964\n",
      "word= waste id= 437\n",
      "word= of id= 7\n",
      "word= my id= 61\n",
      "time. Appended 2\n",
      "word= wish id= 657\n",
      "word= had id= 69\n",
      "word= not id= 24\n",
      "word= seen id= 110\n",
      "word= it id= 12\n",
      "Prediction Probability for  \" the movie was a total waste of my time.  wish had not seen it \" Bi Directional BEST acc RNN-LSTM  =  0.02930437 Sentiment= Negative \n",
      "\n",
      "Prediction Probability for  \" the movie was a total waste of my time.  wish had not seen it \" Bi Directional RNN-LSTM  =  0.4983345 Sentiment= Negative \n",
      "\n",
      "review= that the service lasted only 2 minutes was the only saving grace on the otherwise unknown issue. I will never stop doubting their credentials\n",
      "word= that id= 15\n",
      "word= the id= 4\n",
      "word= service id= 2469\n",
      "word= lasted id= 4341\n",
      "word= only id= 64\n",
      "word= 2 id= 241\n",
      "word= minutes id= 234\n",
      "word= was id= 16\n",
      "word= the id= 4\n",
      "word= only id= 64\n",
      "word= saving id= 1907\n",
      "word= grace id= 1698\n",
      "word= on id= 23\n",
      "word= the id= 4\n",
      "word= otherwise id= 900\n",
      "word= unknown id= 1859\n",
      "issue. Appended 2\n",
      "I Appended 2\n",
      "word= will id= 80\n",
      "word= never id= 115\n",
      "word= stop id= 570\n",
      "got a word outside the vocab_index doubting 17119 breaking\n",
      "Prediction Probability for  \" that the service lasted only 2 minutes was the only saving grace on the otherwise unknown issue. I will never stop doubting their credentials \" Bi Directional BEST acc RNN-LSTM  =  0.02101874 Sentiment= Negative \n",
      "\n",
      "Prediction Probability for  \" that the service lasted only 2 minutes was the only saving grace on the otherwise unknown issue. I will never stop doubting their credentials \" Bi Directional RNN-LSTM  =  0.49699786 Sentiment= Negative \n",
      "\n",
      "review= am unhappy at receiving this, You are so late,               that would have retired by the time got this.   am not going to allow ,                which by the way is a lot at this late date.             frustrated at your courtesy\n",
      "word= am id= 244\n",
      "word= unhappy id= 4434\n",
      "word= at id= 33\n",
      "word= receiving id= 5611\n",
      "this, Appended 2\n",
      "You Appended 2\n",
      "word= are id= 26\n",
      "word= so id= 38\n",
      "late, Appended 2\n",
      "word= that id= 15\n",
      "word= would id= 62\n",
      "word= have id= 28\n",
      "word= retired id= 5046\n",
      "word= by id= 34\n",
      "word= the id= 4\n",
      "word= time id= 58\n",
      "word= got id= 188\n",
      "this. Appended 2\n",
      "word= am id= 244\n",
      "word= not id= 24\n",
      "word= going id= 170\n",
      "word= to id= 8\n",
      "word= allow id= 1741\n",
      ", Appended 2\n",
      "word= which id= 63\n",
      "word= by id= 34\n",
      "word= the id= 4\n",
      "word= way id= 96\n",
      "word= is id= 9\n",
      "word= a id= 6\n",
      "word= lot id= 176\n",
      "word= at id= 33\n",
      "word= this id= 14\n",
      "word= late id= 522\n",
      "date. Appended 2\n",
      "word= frustrated id= 3571\n",
      "word= at id= 33\n",
      "word= your id= 129\n",
      "word= courtesy id= 7558\n",
      "Prediction Probability for  \" am unhappy at receiving this, You are so late,               that would have retired by the time got this.   am not going to allow ,                which by the way is a lot at this late date.             frustrated at your courtesy \" Bi Directional BEST acc RNN-LSTM  =  0.03501049 Sentiment= Negative \n",
      "\n",
      "Prediction Probability for  \" am unhappy at receiving this, You are so late,               that would have retired by the time got this.   am not going to allow ,                which by the way is a lot at this late date.             frustrated at your courtesy \" Bi Directional RNN-LSTM  =  0.50091076 Sentiment= Positive \n",
      "\n",
      "review= You may have reasons to feel pleased about yourself. You would think that  am happy at             receiving this. Unfortunately you are wrong. Very wrong. As Wrong as sky!               not going to allow the amount,                which by the way is a lot  .             Don't be so  happy.Am not and thats what matters! \n",
      "You Appended 2\n",
      "word= may id= 203\n",
      "word= have id= 28\n",
      "word= reasons id= 1007\n",
      "word= to id= 8\n",
      "word= feel id= 235\n",
      "word= pleased id= 3521\n",
      "word= about id= 44\n",
      "yourself. Appended 2\n",
      "You Appended 2\n",
      "word= would id= 62\n",
      "word= think id= 104\n",
      "word= that id= 15\n",
      "word= am id= 244\n",
      "word= happy id= 654\n",
      "word= at id= 33\n",
      "word= receiving id= 5611\n",
      "this. Appended 2\n",
      "Unfortunately Appended 2\n",
      "word= you id= 25\n",
      "word= are id= 26\n",
      "wrong. Appended 2\n",
      "Very Appended 2\n",
      "wrong. Appended 2\n",
      "As Appended 2\n",
      "Wrong Appended 2\n",
      "word= as id= 17\n",
      "sky! Appended 2\n",
      "word= not id= 24\n",
      "word= going id= 170\n",
      "word= to id= 8\n",
      "word= allow id= 1741\n",
      "word= the id= 4\n",
      "amount, Appended 2\n",
      "word= which id= 63\n",
      "word= by id= 34\n",
      "word= the id= 4\n",
      "word= way id= 96\n",
      "word= is id= 9\n",
      "word= a id= 6\n",
      "word= lot id= 176\n",
      ". Appended 2\n",
      "Don't Appended 2\n",
      "word= be id= 30\n",
      "word= so id= 38\n",
      "happy.Am Appended 2\n",
      "word= not id= 24\n",
      "word= and id= 5\n",
      "word= thats id= 1584\n",
      "word= what id= 51\n",
      "matters! Appended 2\n",
      "Prediction Probability for  \" You may have reasons to feel pleased about yourself. You would think that  am happy at             receiving this. Unfortunately you are wrong. Very wrong. As Wrong as sky!               not going to allow the amount,                which by the way is a lot  .             Don't be so  happy.Am not and thats what matters!  \" Bi Directional BEST acc RNN-LSTM  =  0.06629633 Sentiment= Negative \n",
      "\n",
      "Prediction Probability for  \" You may have reasons to feel pleased about yourself. You would think that  am happy at             receiving this. Unfortunately you are wrong. Very wrong. As Wrong as sky!               not going to allow the amount,                which by the way is a lot  .             Don't be so  happy.Am not and thats what matters!  \" Bi Directional RNN-LSTM  =  0.49802077 Sentiment= Negative \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def PredictSentiment(reviewList, ModelListTuple):\n",
    "    sentiment= {True: \"Positive\",\n",
    "               False: \"Negative\"}\n",
    "    Threshold = 0.5\n",
    "    for r in reviewList:\n",
    "        words = r.split()\n",
    "        review = []\n",
    "        print (\"review=\", r)\n",
    "        for word in words:\n",
    "          if word not in word2id: \n",
    "            review.append(2)\n",
    "            print (word, \"Appended 2\")\n",
    "          else:\n",
    "            if (word2id[word]) >= vocabulary_size:\n",
    "                print(\"got a word outside the vocab_index\", word, word2id[word], \"breaking\")\n",
    "                break\n",
    "            print (\"word=\", word, \"id=\", word2id[word])\n",
    "            review.append(word2id[word]) \n",
    "        review = keras.preprocessing.sequence.pad_sequences([review],\n",
    "          truncating='pre', padding='pre', maxlen=max_words)\n",
    "        for i,m in enumerate(ModelListTuple):\n",
    "            if m[0] is not None:\n",
    "                prediction = m[0].predict(review)\n",
    "                print(\"Prediction Probability for \", \"\\\"\",r, \"\\\"\",ModelListTuple[i][1],\" = \", prediction[0][0], \"Sentiment=\", \n",
    "                      sentiment[prediction[0][0]>Threshold], \"\\n\")\n",
    "                \n",
    "PredictSentiment(reviewList,[(modelBestBi, \"Bi Directional BEST acc RNN-LSTM\"), (modelBi, \"Bi Directional RNN-LSTM\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
