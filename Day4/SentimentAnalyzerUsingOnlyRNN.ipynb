{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This demonstrates the following:\n",
    "How to load imdb data\n",
    "How load converts words into integer indexes\n",
    "How words are stored in highest occuring freq to lowest occurring\n",
    "How to take top 5000 or 10000 vocabulary size\n",
    "idiosyncracy of imdb.get_word_index() \n",
    "Re-convert the index to take into account padding, start, out-of-vocabulary\n",
    "What is an embedding layer\n",
    "Create a RNN model\n",
    "DO the sentiment analyzer\n",
    "Predict sample sentences\n",
    "This also takes into advanced features of Keras such as callbacks and loads the best saved weights also"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python36\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.datasets import imdb\n",
    "from keras.layers import Dense,  Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.4\n"
     ]
    }
   ],
   "source": [
    "print (keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMDB dataset  25000 training samples, 25000 test samples\n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = 10000\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words = vocabulary_size)\n",
    "print('IMDB dataset  {} training samples, {} test samples'.format(len(X_train), len(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88584 id2Word[1] = the w2id[the] 1\n",
      "id2Word[2] = and w2id[and] 2\n",
      "Least frequently ocurring word is at index 88583 and it is =  voorhees'\n"
     ]
    }
   ],
   "source": [
    "w2id = imdb.get_word_index() \n",
    "#GET_WORD_INDEX gets the dictionary where the occurring words are stored. It does not store tokens like \"Start\", \"unknown\" etc.\n",
    "#We need to make that correction.\n",
    "id2word = {i: word  for word, i in w2id.items()}\n",
    "print (len(w2id), \"id2Word[1] =\", id2word[1], \"w2id[the]\" ,w2id[\"the\"])\n",
    "print ( \"id2Word[2] =\", id2word[2], \"w2id[and]\" ,w2id[\"and\"])\n",
    "print (\"Least frequently ocurring word is at index 88583 and it is = \", id2word[88583])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the out can't some to hit looking when is themselves novel as it theatre people have former some br is on dull this should rick to and thru characters one lost her show jews for of brosnan br realised back video excellent to plenty theatre involved and and off as in we no that how moments some to as on it early in we years was nothing does is 10 has as new part first that to when is possibly what have whatever of positive have 2 also of slow for as it by br though and characters has so heart broad europe film show of try making to looks in one want key bodies br when is him their that it christmas not them his dancer want this and as it his for on of great stupid of their as end should is very together br twilight us writer br were machines go bad aka were young to might us father film any more it her get predictable of alleged for find dvd from through i'm her get up happily date of lionel with other is him pushed has columbo criticized to what's of thief this so superior too not as you it get up br is others their was out least that hilarious not was into through to why for as it by br of where suits was one your life len= 1137 \n",
      "And sentiment is  1\n",
      "[1, 43, 188, 46, 5, 566, 264, 51, 6, 530, 664, 14, 9, 1713, 81, 25, 1135, 46, 7, 6, 20, 750, 11, 141, 4299, 5, 2, 4441, 102, 28, 413, 38, 120, 5533, 15, 4, 3974, 7, 5369, 142, 371, 318, 5, 955, 1713, 571, 2, 2, 122, 14, 8, 72, 54, 12, 86, 385, 46, 5, 14, 20, 9, 399, 8, 72, 150, 13, 161, 124, 6, 155, 44, 14, 159, 170, 83, 12, 5, 51, 6, 866, 48, 25, 842, 4, 1120, 25, 238, 79, 4, 547, 15, 14, 9, 31, 7, 148, 2, 102, 44, 35, 480, 3823, 2380, 19, 120, 4, 350, 228, 5, 269, 8, 28, 178, 1314, 2347, 7, 51, 6, 87, 65, 12, 9, 979, 21, 95, 24, 3186, 178, 11, 2, 14, 9, 24, 15, 20, 4, 84, 376, 4, 65, 14, 127, 141, 6, 52, 292, 7, 4751, 175, 561, 7, 68, 3866, 137, 75, 2541, 68, 182, 5, 235, 175, 333, 19, 98, 50, 9, 38, 76, 724, 4, 6750, 15, 166, 285, 36, 140, 143, 38, 76, 53, 3094, 1301, 4, 6991, 16, 82, 6, 87, 3578, 44, 2527, 7612, 5, 800, 4, 3033, 11, 35, 1728, 96, 21, 14, 22, 9, 76, 53, 7, 6, 406, 65, 13, 43, 219, 12, 639, 21, 13, 80, 140, 5, 135, 15, 14, 9, 31, 7, 4, 118, 3672, 13, 28, 126, 110] len= 233 \n",
      "And sentiment is  1\n"
     ]
    }
   ],
   "source": [
    "#gibberish data?\n",
    "actual_data = []\n",
    "for i in range (len(X_train[8])):\n",
    "    ind = X_train[8][i]\n",
    "    if(ind != 0):\n",
    "        actual_data.append(id2word[ind])\n",
    "actual_data = \" \".join (actual_data )\n",
    "print (actual_data, \"len=\", len(actual_data),\"\\nAnd sentiment is \", y_train[8])\n",
    "print (X_train[8], \"len=\", len(X_train[8]),\"\\nAnd sentiment is \", y_train[8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IN THIS EXAMPLE WE GOT THE IMDB TO LOAD THE DATASET IN A PECULIAR WAY. ON LOADING IT ASSUMES THAT 1ST WORD IS \"START\" TOKEN, 2ND WORD IS \"UNKNOWN\" TOKEN. BUT IT  STORES THEM IN ORDER OF THE FREQUENCY OF OCCURING. SO MOST FREQUENT WORD WILL BE STORED IN INDEX 1 (THAT CORRESPONDS TO \"THE\"), LESS FREQUENT IN INDEX 2, AND SO ON. IT RESERVES 0 FOR PADDING. So THE WORDS \"UNKONWN\", \"START\"ARE NOT STORED YET. SO A PARTICULAR TRAINING DATA  ROW INDEX REFERS TO ACTUAL INDEX + 2. HENCE WE MAKE THAT CORRECTION."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'> 88587\n",
      "__START__ __UNK__ the and a voorhees'\n",
      "__START__ just got out and cannot believe what a brilliant documentary this is rarely do you walk out of a movie theater in such awe and __UNK__ lately movies have become so over hyped that the thrill of discovering something truly special and unique rarely happens __UNK__ __UNK__ did this to me when it first came out and this movie is doing to me now i didn't know a thing about this before going into it and what a surprise if you hear the concept you might get the feeling that this is one of those __UNK__ movies about an amazing triumph covered with over the top music and trying to have us fully convinced of what a great story it is telling but then not letting us in __UNK__ this is not that movie the people tell the story this does such a good job of capturing every moment of their involvement while we enter their world and feel every second with them there is so much beyond the climb that makes everything they go through so much more tense touching the void was also a great doc about mountain climbing and showing the intensity in an engaging way but this film is much more of a human story i just saw it today but i will go and say that this is one of the best documentaries i have ever seen \n",
      "And sentiment is  1\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "#So we make a new dictionary offsetting by 3, keeping one for out of vocabulary in future\n",
    "word2id ={w: i+3 for w, i in w2id.items()}\n",
    "word2id[\"__PADDING__\"] = 0\n",
    "word2id[\"__START__\"] = 1\n",
    "word2id[\"__UNK__\"] = 2\n",
    "\n",
    "#This returns the index of the words from 1 to n with 1 being the most frequently occuring word, \n",
    "\n",
    "# and n the least frequently occuring word\n",
    "\n",
    "print (type(word2id), len (word2id))\n",
    "\n",
    "id2word = {i: word  for word, i in word2id.items()}\n",
    "print(id2word[1], id2word[2], #id2word[3],#\n",
    "      id2word[4], id2word[5], id2word[6], id2word[88586])\n",
    "actual_data = []\n",
    "for i in range (len(X_train[8])):\n",
    "    ind = X_train[8][i]\n",
    "    if(ind != 0):\n",
    "        actual_data.append(id2word[ind])\n",
    "actual_data = \" \".join (actual_data )\n",
    "print (actual_data, \"\\nAnd sentiment is \", y_train[8])\n",
    "print (word2id[\"the\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum review length: 2697\n"
     ]
    }
   ],
   "source": [
    "print('Maximum review length: {}'.format(\n",
    "len((max((X_train + X_test), key=len)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Padding X[train[8]=\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    1   43  188   46    5  566  264   51    6  530  664   14    9\n",
      " 1713   81   25 1135   46    7    6   20  750   11  141 4299    5    2\n",
      " 4441  102   28  413   38  120 5533   15    4 3974    7 5369  142  371\n",
      "  318    5  955 1713  571    2    2  122   14    8   72   54   12   86\n",
      "  385   46    5   14   20    9  399    8   72  150   13  161  124    6\n",
      "  155   44   14  159  170   83   12    5   51    6  866   48   25  842\n",
      "    4 1120   25  238   79    4  547   15   14    9   31    7  148    2\n",
      "  102   44   35  480 3823 2380   19  120    4  350  228    5  269    8\n",
      "   28  178 1314 2347    7   51    6   87   65   12    9  979   21   95\n",
      "   24 3186  178   11    2   14    9   24   15   20    4   84  376    4\n",
      "   65   14  127  141    6   52  292    7 4751  175  561    7   68 3866\n",
      "  137   75 2541   68  182    5  235  175  333   19   98   50    9   38\n",
      "   76  724    4 6750   15  166  285   36  140  143   38   76   53 3094\n",
      " 1301    4 6991   16   82    6   87 3578   44 2527 7612    5  800    4\n",
      " 3033   11   35 1728   96   21   14   22    9   76   53    7    6  406\n",
      "   65   13   43  219   12  639   21   13   80  140    5  135   15   14\n",
      "    9   31    7    4  118 3672   13   28  126  110]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import sequence\n",
    "max_words = 500\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_words)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_words)\n",
    "modelCNN = None\n",
    "modelRNN=None\n",
    "modelLSTM=None\n",
    "print (\"After Padding X[train[8]=\\n\", X_train[8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now create a simple RNN model and lets see the accuracy\n",
    "from keras.layers import SimpleRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 500, 50)           500000    \n",
      "_________________________________________________________________\n",
      "simple_rnn_1 (SimpleRNN)     (None, 150)               30150     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 151       \n",
      "=================================================================\n",
      "Total params: 530,301\n",
      "Trainable params: 530,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "embedding_size=50\n",
    "modelRNN=Sequential()\n",
    "modelRNN.add(Embedding(vocabulary_size, embedding_size, input_length=max_words))\n",
    "modelRNN.add(SimpleRNN(150,activation = \"tanh\")) \n",
    "modelRNN.add(Dense(1, activation='sigmoid'))\n",
    "print(modelRNN.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\python36\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3376: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\python36\\lib\\site-packages\\tensorflow_core\\python\\ops\\nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "modelRNN.compile(loss='binary_crossentropy', \n",
    "             optimizer='adam', \n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "num_epochs = 20\n",
    "X_valid, y_valid = X_train[:batch_size], y_train[:batch_size]\n",
    "X_train2, y_train2 = X_train[batch_size:], y_train[batch_size:]\n",
    "callback_listRNN = [#early1\n",
    "                keras.callbacks.ModelCheckpoint(filepath=\"my_modRNN_BestValAcc.h5\", monitor=\"val_acc\",\n",
    "                                               save_best_only=True),\n",
    "                #keras.callbacks.TerminateOnNaN()\n",
    "                ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:2741: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "Train on 24744 samples, validate on 256 samples\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:From c:\\python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "24744/24744 [==============================] - 60s 2ms/step - loss: 0.6868 - acc: 0.5441 - val_loss: 0.6781 - val_acc: 0.5938\n",
      "Epoch 2/20\n",
      "24744/24744 [==============================] - 61s 2ms/step - loss: 0.6407 - acc: 0.6505 - val_loss: 0.6412 - val_acc: 0.6055\n",
      "Epoch 3/20\n",
      "24744/24744 [==============================] - 59s 2ms/step - loss: 0.5629 - acc: 0.7197 - val_loss: 0.5523 - val_acc: 0.7109\n",
      "Epoch 4/20\n",
      "24744/24744 [==============================] - 83s 3ms/step - loss: 0.4631 - acc: 0.7804 - val_loss: 0.5794 - val_acc: 0.7148\n",
      "Epoch 5/20\n",
      "24744/24744 [==============================] - 94s 4ms/step - loss: 0.4387 - acc: 0.8054 - val_loss: 0.9088 - val_acc: 0.5625\n",
      "Epoch 6/20\n",
      "24744/24744 [==============================] - 90s 4ms/step - loss: 0.4271 - acc: 0.8031 - val_loss: 0.6012 - val_acc: 0.7227\n",
      "Epoch 7/20\n",
      "24744/24744 [==============================] - 90s 4ms/step - loss: 0.3326 - acc: 0.8614 - val_loss: 0.5497 - val_acc: 0.7344\n",
      "Epoch 8/20\n",
      "24744/24744 [==============================] - 94s 4ms/step - loss: 0.3338 - acc: 0.8617 - val_loss: 0.6832 - val_acc: 0.6055\n",
      "Epoch 9/20\n",
      "24744/24744 [==============================] - 85s 3ms/step - loss: 0.4992 - acc: 0.7419 - val_loss: 0.6803 - val_acc: 0.6523\n",
      "Epoch 10/20\n",
      "24744/24744 [==============================] - 84s 3ms/step - loss: 0.3953 - acc: 0.8307 - val_loss: 0.7171 - val_acc: 0.5625\n",
      "Epoch 11/20\n",
      "24744/24744 [==============================] - 81s 3ms/step - loss: 0.5114 - acc: 0.7298 - val_loss: 0.7588 - val_acc: 0.6094\n",
      "Epoch 12/20\n",
      "24744/24744 [==============================] - 87s 4ms/step - loss: 0.4383 - acc: 0.7874 - val_loss: 0.7244 - val_acc: 0.6055\n",
      "Epoch 13/20\n",
      "24744/24744 [==============================] - 92s 4ms/step - loss: 0.3921 - acc: 0.8221 - val_loss: 0.7683 - val_acc: 0.6406\n",
      "Epoch 14/20\n",
      "24744/24744 [==============================] - 88s 4ms/step - loss: 0.3013 - acc: 0.8712 - val_loss: 0.7617 - val_acc: 0.6992\n",
      "Epoch 15/20\n",
      "24744/24744 [==============================] - 89s 4ms/step - loss: 0.2343 - acc: 0.9060 - val_loss: 0.7779 - val_acc: 0.7070\n",
      "Epoch 16/20\n",
      "24744/24744 [==============================] - 85s 3ms/step - loss: 0.2085 - acc: 0.9199 - val_loss: 0.7876 - val_acc: 0.7227\n",
      "Epoch 17/20\n",
      "24744/24744 [==============================] - 70s 3ms/step - loss: 0.1960 - acc: 0.9249 - val_loss: 0.8546 - val_acc: 0.7266\n",
      "Epoch 18/20\n",
      "24744/24744 [==============================] - 68s 3ms/step - loss: 0.1641 - acc: 0.9407 - val_loss: 0.8967 - val_acc: 0.7227\n",
      "Epoch 19/20\n",
      "24744/24744 [==============================] - 83s 3ms/step - loss: 0.1439 - acc: 0.9487 - val_loss: 0.9919 - val_acc: 0.7070\n",
      "Epoch 20/20\n",
      "24744/24744 [==============================] - 78s 3ms/step - loss: 0.1200 - acc: 0.9584 - val_loss: 0.9247 - val_acc: 0.7578\n"
     ]
    }
   ],
   "source": [
    "modelRNN.fit(X_train2, y_train2, validation_data=(X_valid, y_valid), batch_size=batch_size, epochs=num_epochs, \n",
    "             callbacks=callback_listRNN)\n",
    "modelRNN.save_weights(\"my_modRNN_Latest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.74372\n"
     ]
    }
   ],
   "source": [
    "scoresRNN = modelRNN.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test accuracy:', scoresRNN[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 500, 50)           500000    \n",
      "_________________________________________________________________\n",
      "simple_rnn_2 (SimpleRNN)     (None, 150)               30150     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 151       \n",
      "=================================================================\n",
      "Total params: 530,301\n",
      "Trainable params: 530,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#Loading the last epoch model\n",
    "modelRNN=Sequential()\n",
    "modelRNN.add(Embedding(vocabulary_size, embedding_size, input_length=max_words))\n",
    "modelRNN.add(SimpleRNN(150,activation = \"tanh\")) \n",
    "modelRNN.add(Dense(1, activation='sigmoid'))\n",
    "print(modelRNN.summary())\n",
    "modelRNN.compile(loss='binary_crossentropy', \n",
    "             optimizer='adam', \n",
    "             metrics=['accuracy'])\n",
    "modelRNN.load_weights(\"my_modRNN_Latest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 500, 50)           500000    \n",
      "_________________________________________________________________\n",
      "simple_rnn_3 (SimpleRNN)     (None, 150)               30150     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 151       \n",
      "=================================================================\n",
      "Total params: 530,301\n",
      "Trainable params: 530,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "[0.902381838722229, 0.74372]\n"
     ]
    }
   ],
   "source": [
    "bestModel=Sequential()\n",
    "bestModel.add(Embedding(vocabulary_size, embedding_size, input_length=max_words))\n",
    "bestModel.add(SimpleRNN(150,activation = \"tanh\")) \n",
    "bestModel.add(Dense(1, activation='sigmoid'))\n",
    "print(bestModel.summary())\n",
    "bestModel.compile(loss='binary_crossentropy', \n",
    "             optimizer='adam', \n",
    "             metrics=['accuracy'])\n",
    "bestModel.load_weights(\"my_modRNN_BestValAcc.h5\")\n",
    "print (bestModel.evaluate(X_test, y_test, verbose=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now going to predict these reviews\n"
     ]
    }
   ],
   "source": [
    "reviewList = [\"the movie was boring\",\n",
    "              \"the movie was not too long\",\n",
    "              \"the movie was a total waste of my time\",\n",
    "              \"it was a lovely evening\",\n",
    "              \"the cat was adorable\",\n",
    "              \"such pusilamity was not encountered before\"\n",
    "             ]\n",
    "print (\"Now going to predict these reviews\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "print (word2id[\"the\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "review= the movie was boring\n",
      "word= the id= 4\n",
      "word= movie id= 20\n",
      "word= was id= 16\n",
      "word= boring id= 357\n",
      "Prediction Probability for  \" the movie was boring \" RNN  =  0.04350068 Sentiment= Negative \n",
      "\n",
      "Prediction Probability for  \" the movie was boring \" Best Val Checkpointed Model  =  0.04350068 Sentiment= Negative \n",
      "\n",
      "review= the movie was not too long\n",
      "word= the id= 4\n",
      "word= movie id= 20\n",
      "word= was id= 16\n",
      "word= not id= 24\n",
      "word= too id= 99\n",
      "word= long id= 196\n",
      "Prediction Probability for  \" the movie was not too long \" RNN  =  0.0046611633 Sentiment= Negative \n",
      "\n",
      "Prediction Probability for  \" the movie was not too long \" Best Val Checkpointed Model  =  0.0046611633 Sentiment= Negative \n",
      "\n",
      "review= the movie was a total waste of my time\n",
      "word= the id= 4\n",
      "word= movie id= 20\n",
      "word= was id= 16\n",
      "word= a id= 6\n",
      "word= total id= 964\n",
      "word= waste id= 437\n",
      "word= of id= 7\n",
      "word= my id= 61\n",
      "word= time id= 58\n",
      "Prediction Probability for  \" the movie was a total waste of my time \" RNN  =  0.03787117 Sentiment= Negative \n",
      "\n",
      "Prediction Probability for  \" the movie was a total waste of my time \" Best Val Checkpointed Model  =  0.03787117 Sentiment= Negative \n",
      "\n",
      "review= it was a lovely evening\n",
      "word= it id= 12\n",
      "word= was id= 16\n",
      "word= a id= 6\n",
      "word= lovely id= 1334\n",
      "word= evening id= 2193\n",
      "Prediction Probability for  \" it was a lovely evening \" RNN  =  0.04349562 Sentiment= Negative \n",
      "\n",
      "Prediction Probability for  \" it was a lovely evening \" Best Val Checkpointed Model  =  0.04349562 Sentiment= Negative \n",
      "\n",
      "review= the cat was adorable\n",
      "word= the id= 4\n",
      "word= cat id= 1132\n",
      "word= was id= 16\n",
      "word= adorable id= 4276\n",
      "Prediction Probability for  \" the cat was adorable \" RNN  =  0.96615165 Sentiment= Positive \n",
      "\n",
      "Prediction Probability for  \" the cat was adorable \" Best Val Checkpointed Model  =  0.96615165 Sentiment= Positive \n",
      "\n",
      "review= such pusilamity was not encountered before\n",
      "word= such id= 141\n",
      "pusilamity Appended 2\n",
      "word= was id= 16\n",
      "word= not id= 24\n",
      "word= encountered id= 6967\n",
      "word= before id= 159\n",
      "Prediction Probability for  \" such pusilamity was not encountered before \" RNN  =  0.023511488 Sentiment= Negative \n",
      "\n",
      "Prediction Probability for  \" such pusilamity was not encountered before \" Best Val Checkpointed Model  =  0.023511488 Sentiment= Negative \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def PredictSentiment(reviewList, ModelListTuple):\n",
    "    sentiment= {True: \"Positive\",\n",
    "               False: \"Negative\"}\n",
    "    Threshold = 0.5\n",
    "    for r in reviewList:\n",
    "        words = r.split()\n",
    "        review = []\n",
    "        print (\"review=\", r)\n",
    "        for word in words:\n",
    "          if word not in word2id: \n",
    "            review.append(2)\n",
    "            print (word, \"Appended 2\")\n",
    "          else:\n",
    "            if (word2id[word]) >= vocabulary_size:\n",
    "                print(\"got a word outside the vocab_index\", word, word2id[word], \"replacing with unk\")               \n",
    "                review.append(2) \n",
    "            else:\n",
    "                print (\"word=\", word, \"id=\", word2id[word])\n",
    "                review.append(word2id[word]) \n",
    "        review = keras.preprocessing.sequence.pad_sequences([review],\n",
    "          truncating='pre', padding='pre', maxlen=max_words)\n",
    "        for i,m in enumerate(ModelListTuple):\n",
    "            if m[0] is not None:\n",
    "                prediction = m[0].predict(review)\n",
    "                print(\"Prediction Probability for \", \"\\\"\",r, \"\\\"\",ModelListTuple[i][1],\" = \", prediction[0][0], \"Sentiment=\", \n",
    "                      sentiment[prediction[0][0]>Threshold], \"\\n\")\n",
    "                \n",
    "PredictSentiment(reviewList,[(modelRNN, \"RNN\"), (bestModel, \"Best Val Checkpointed Model\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "review= I am unhappy at receiving this request. You are so late in application,               that I would have retired by the time I got this mail.  I am not going to allow the amount                (which by the way is a lot at this late date.             But I am frustrated at your courtesy\n",
      "I Appended 2\n",
      "word= am id= 244\n",
      "word= unhappy id= 4434\n",
      "word= at id= 33\n",
      "word= receiving id= 5611\n",
      "word= this id= 14\n",
      "request. Appended 2\n",
      "You Appended 2\n",
      "word= are id= 26\n",
      "word= so id= 38\n",
      "word= late id= 522\n",
      "word= in id= 11\n",
      "application, Appended 2\n",
      "word= that id= 15\n",
      "I Appended 2\n",
      "word= would id= 62\n",
      "word= have id= 28\n",
      "word= retired id= 5046\n",
      "word= by id= 34\n",
      "word= the id= 4\n",
      "word= time id= 58\n",
      "I Appended 2\n",
      "word= got id= 188\n",
      "word= this id= 14\n",
      "mail. Appended 2\n",
      "I Appended 2\n",
      "word= am id= 244\n",
      "word= not id= 24\n",
      "word= going id= 170\n",
      "word= to id= 8\n",
      "word= allow id= 1741\n",
      "word= the id= 4\n",
      "word= amount id= 1166\n",
      "(which Appended 2\n",
      "word= by id= 34\n",
      "word= the id= 4\n",
      "word= way id= 96\n",
      "word= is id= 9\n",
      "word= a id= 6\n",
      "word= lot id= 176\n",
      "word= at id= 33\n",
      "word= this id= 14\n",
      "word= late id= 522\n",
      "date. Appended 2\n",
      "But Appended 2\n",
      "I Appended 2\n",
      "word= am id= 244\n",
      "word= frustrated id= 3571\n",
      "word= at id= 33\n",
      "word= your id= 129\n",
      "word= courtesy id= 7558\n",
      "Prediction Probability for  \" I am unhappy at receiving this request. You are so late in application,               that I would have retired by the time I got this mail.  I am not going to allow the amount                (which by the way is a lot at this late date.             But I am frustrated at your courtesy \" RNN  =  0.6398589 Sentiment= Positive \n",
      "\n",
      "Prediction Probability for  \" I am unhappy at receiving this request. You are so late in application,               that I would have retired by the time I got this mail.  I am not going to allow the amount                (which by the way is a lot at this late date.             But I am frustrated at your courtesy \" Best Val Checkpointed Model  =  0.12741879 Sentiment= Negative \n",
      "\n",
      "review= You may have reasons to feel pleased about yourself. You would think that I am happy at             receiving this request. Unfortunately you are wrong. Very wrong. As Wrong as sky is black!             You are so late in application,               that I would have retired by the time I got this mail.  I am not going to allow the amount                (which by the way is a lot at this late date.             Don't think that I do not know you are happy. But I am not and thats what matters! \n",
      "You Appended 2\n",
      "word= may id= 203\n",
      "word= have id= 28\n",
      "word= reasons id= 1007\n",
      "word= to id= 8\n",
      "word= feel id= 235\n",
      "word= pleased id= 3521\n",
      "word= about id= 44\n",
      "yourself. Appended 2\n",
      "You Appended 2\n",
      "word= would id= 62\n",
      "word= think id= 104\n",
      "word= that id= 15\n",
      "I Appended 2\n",
      "word= am id= 244\n",
      "word= happy id= 654\n",
      "word= at id= 33\n",
      "word= receiving id= 5611\n",
      "word= this id= 14\n",
      "request. Appended 2\n",
      "Unfortunately Appended 2\n",
      "word= you id= 25\n",
      "word= are id= 26\n",
      "wrong. Appended 2\n",
      "Very Appended 2\n",
      "wrong. Appended 2\n",
      "As Appended 2\n",
      "Wrong Appended 2\n",
      "word= as id= 17\n",
      "word= sky id= 1749\n",
      "word= is id= 9\n",
      "black! Appended 2\n",
      "You Appended 2\n",
      "word= are id= 26\n",
      "word= so id= 38\n",
      "word= late id= 522\n",
      "word= in id= 11\n",
      "application, Appended 2\n",
      "word= that id= 15\n",
      "I Appended 2\n",
      "word= would id= 62\n",
      "word= have id= 28\n",
      "word= retired id= 5046\n",
      "word= by id= 34\n",
      "word= the id= 4\n",
      "word= time id= 58\n",
      "I Appended 2\n",
      "word= got id= 188\n",
      "word= this id= 14\n",
      "mail. Appended 2\n",
      "I Appended 2\n",
      "word= am id= 244\n",
      "word= not id= 24\n",
      "word= going id= 170\n",
      "word= to id= 8\n",
      "word= allow id= 1741\n",
      "word= the id= 4\n",
      "word= amount id= 1166\n",
      "(which Appended 2\n",
      "word= by id= 34\n",
      "word= the id= 4\n",
      "word= way id= 96\n",
      "word= is id= 9\n",
      "word= a id= 6\n",
      "word= lot id= 176\n",
      "word= at id= 33\n",
      "word= this id= 14\n",
      "word= late id= 522\n",
      "date. Appended 2\n",
      "Don't Appended 2\n",
      "word= think id= 104\n",
      "word= that id= 15\n",
      "I Appended 2\n",
      "word= do id= 81\n",
      "word= not id= 24\n",
      "word= know id= 124\n",
      "word= you id= 25\n",
      "word= are id= 26\n",
      "happy. Appended 2\n",
      "But Appended 2\n",
      "I Appended 2\n",
      "word= am id= 244\n",
      "word= not id= 24\n",
      "word= and id= 5\n",
      "word= thats id= 1584\n",
      "word= what id= 51\n",
      "matters! Appended 2\n",
      "Prediction Probability for  \" You may have reasons to feel pleased about yourself. You would think that I am happy at             receiving this request. Unfortunately you are wrong. Very wrong. As Wrong as sky is black!             You are so late in application,               that I would have retired by the time I got this mail.  I am not going to allow the amount                (which by the way is a lot at this late date.             Don't think that I do not know you are happy. But I am not and thats what matters!  \" RNN  =  0.5010009 Sentiment= Positive \n",
      "\n",
      "Prediction Probability for  \" You may have reasons to feel pleased about yourself. You would think that I am happy at             receiving this request. Unfortunately you are wrong. Very wrong. As Wrong as sky is black!             You are so late in application,               that I would have retired by the time I got this mail.  I am not going to allow the amount                (which by the way is a lot at this late date.             Don't think that I do not know you are happy. But I am not and thats what matters!  \" Best Val Checkpointed Model  =  0.80301565 Sentiment= Positive \n",
      "\n"
     ]
    }
   ],
   "source": [
    "reviewList = [             \"I am unhappy at receiving this request. You are so late in application, \\\n",
    "              that I would have retired by the time I got this mail.  I am not going to allow the amount  \\\n",
    "              (which by the way is a lot at this late date.\\\n",
    "             But I am frustrated at your courtesy\",\n",
    "            \"You may have reasons to feel pleased about yourself. You would think that I am happy at \\\n",
    "            receiving this request. Unfortunately you are wrong. Very wrong. As Wrong as sky is black! \\\n",
    "            You are so late in application, \\\n",
    "              that I would have retired by the time I got this mail.  I am not going to allow the amount  \\\n",
    "              (which by the way is a lot at this late date.\\\n",
    "             Don't think that I do not know you are happy. But I am not and thats what matters! \"              \n",
    "\n",
    "             ]\n",
    "PredictSentiment(reviewList,[(modelRNN, \"RNN\"), (bestModel, \"Best Val Checkpointed Model\")])\n",
    "#We will try this in LSTM and BI Directional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
