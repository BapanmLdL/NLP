{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This demonstrates the following:\n",
    "How to load imdb data\n",
    "How load converts words into integer indexes\n",
    "How words are stored in highest occuring freq to lowest occurring\n",
    "How to take top 5000 or 10000 vocbualry size\n",
    "idiosyncracy of imdb.get_word_index() \n",
    "Re-convert the index to take into account padding, start, out-of-vocabulary\n",
    "Create a CNN wtih multiple kernel size (corresponding to 1-gram, 2-gram, 3-gram)\n",
    "Use functional Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python36\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.11.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print (tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.datasets import imdb\n",
    "from keras.layers import Dense,  Flatten\n",
    "from keras.layers import Conv1D, Conv2D, MaxPooling1D,MaxPooling2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.4\n",
      "IMDB dataset  25000 training samples, 25000 test samples\n"
     ]
    }
   ],
   "source": [
    "print (keras.__version__)\n",
    "vocabulary_size = 10000\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words = vocabulary_size)\n",
    "print('IMDB dataset  {} training samples, {} test samples'.format(len(X_train), len(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Features: review in number sequence After padding---\n",
      "[1, 6740, 365, 1234, 5, 1156, 354, 11, 14, 5327, 6638, 7, 1016, 2, 5940, 356, 44, 4, 1349, 500, 746, 5, 200, 4, 4132, 11, 2, 9363, 1117, 1831, 7485, 5, 4831, 26, 6, 2, 4183, 17, 369, 37, 215, 1345, 143, 2, 5, 1838, 8, 1974, 15, 36, 119, 257, 85, 52, 486, 9, 6, 2, 8564, 63, 271, 6, 196, 96, 949, 4121, 4, 2, 7, 4, 2212, 2436, 819, 63, 47, 77, 7175, 180, 6, 227, 11, 94, 2494, 2, 13, 423, 4, 168, 7, 4, 22, 5, 89, 665, 71, 270, 56, 5, 13, 197, 12, 161, 5390, 99, 76, 23, 2, 7, 419, 665, 40, 91, 85, 108, 7, 4, 2084, 5, 4773, 81, 55, 52, 1901]\n",
      "Sentiment 1 = positve, 0 = negative\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print('---Features: review in number sequence After padding---')\n",
    "print(X_train[6])\n",
    "print('Sentiment 1 = positve, 0 = negative')\n",
    "print(y_train[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88584 the 1\n"
     ]
    }
   ],
   "source": [
    "w2id = imdb.get_word_index() \n",
    "id2word = {i: word  for word, i in w2id.items()}\n",
    "print (len(w2id), id2word[1], w2id[\"the\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the boiled full involving to impressive boring this as murdering naschy br villain and suggestion need has of costumes b message to may of props this and concentrates concept issue skeptical to god's he is and unfolds movie women like isn't surely i'm and to toward in here's for from did having because very quality it is and starship really book is both too worked carl of and br of reviewer closer figure really there will originals things is far this make mistakes and was couldn't of few br of you to don't female than place she to was between that nothing dose movies get are and br yes female just its because many br of overly to descent people time very bland \n",
      "And sentiment is  1\n"
     ]
    }
   ],
   "source": [
    "#Jibberish data?\n",
    "actual_data = []\n",
    "for i in range (len(X_train[6])):\n",
    "    ind = X_train[6][i]\n",
    "    if(ind != 0):\n",
    "        actual_data.append(id2word[ind])\n",
    "actual_data = \" \".join (actual_data )\n",
    "print (actual_data, \"\\nAnd sentiment is \", y_train[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'> 88587\n",
      "__START__ __UNK__ the and a voorhees'\n",
      "__START__ lavish production values and solid performances in this straightforward adaption of jane __UNK__ satirical classic about the marriage game within and between the classes in __UNK__ 18th century england northam and paltrow are a __UNK__ mixture as friends who must pass through __UNK__ and lies to discover that they love each other good humor is a __UNK__ virtue which goes a long way towards explaining the __UNK__ of the aged source material which has been toned down a bit in its harsh __UNK__ i liked the look of the film and how shots were set up and i thought it didn't rely too much on __UNK__ of head shots like most other films of the 80s and 90s do very good results \n",
      "And sentiment is  1\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "word2id ={w: i+3 for w, i in w2id.items()}\n",
    "word2id[\"__PADDING__\"] = 0\n",
    "word2id[\"__START__\"] = 1\n",
    "word2id[\"__UNK__\"] = 2\n",
    "\n",
    "#This returns the index of the words from 1 to n with 1 being the most frequently occuring word, \n",
    "\n",
    "# and n the least frequently occuring word\n",
    "\n",
    "print (type(word2id), len (word2id))\n",
    "\n",
    "id2word = {i: word  for word, i in word2id.items()}\n",
    "print(id2word[1], id2word[2], #id2word[3],#\n",
    "      id2word[4], id2word[5], id2word[6], id2word[88586])\n",
    "actual_data = []\n",
    "for i in range (len(X_train[6])):\n",
    "    ind = X_train[6][i]\n",
    "    if(ind != 0):\n",
    "        actual_data.append(id2word[ind])\n",
    "actual_data = \" \".join (actual_data )\n",
    "print (actual_data, \"\\nAnd sentiment is \", y_train[6])\n",
    "print (word2id[\"the\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum review length: 2697\n"
     ]
    }
   ],
   "source": [
    "print('Maximum review length: {}'.format(\n",
    "len((max((X_train + X_test), key=len)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "max_words = 500\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_words)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_words)\n",
    "modelCNN = None\n",
    "modelRNN=None\n",
    "modelLSTM=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout,  Concatenate, MaxPool2D, MaxPool1D, Reshape,GlobalMaxPooling1D\n",
    "from keras.models import Sequential,Model\n",
    "from keras import Input\n",
    "from keras.layers.advanced_activations import ReLU\n",
    "from keras.layers import Activation\n",
    "from keras import regularizers\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CreateModel2D3Nets():\n",
    "    #embedding_size=32\n",
    "\n",
    "    embedding_size=50\n",
    "    deep_inputs = Input(shape=(max_words,))\n",
    "    print (\"deep_inputs.shape\", deep_inputs.shape)\n",
    "    x = Embedding(vocabulary_size, embedding_size, input_length=max_words)(deep_inputs)\n",
    "    x = Reshape((max_words,embedding_size,1) )(x)\n",
    "    print (\"x.shape\", x.shape)\n",
    "#     x = np.reshape(max_words, embedding_size, 1)(x)\n",
    "#     print (x.shape)\n",
    "    x1  = Conv2D(filters=100, kernel_size=(5, embedding_size),  activation=\"relu\")(x)\n",
    "    print (\"x1.shape\", x1.shape)\n",
    "    x1Pool = MaxPool2D (pool_size=(max_words-4, 1))(x1)    \n",
    "    print (\"x1Pool.shape\", x1Pool.shape) \n",
    "    x2 =  Conv2D(filters=100, kernel_size=(2, embedding_size),  activation=\"relu\")(x)\n",
    "    print (\"x2.shape\", x2.shape)\n",
    "    x2Pool = MaxPool2D (pool_size=(max_words-1, 1))(x2)    \n",
    "    print (\"x2Pool.shape\", x2Pool.shape)    \n",
    "    x3 =  Conv2D(filters=100, kernel_size=(3, embedding_size),  activation=\"relu\")(x)\n",
    "    print (\"x3.shape\", x3.shape)\n",
    "    x3Pool = MaxPool2D (pool_size=(max_words-2, 1))(x3)    \n",
    "    print (\"x3Pool.shape\", x3Pool.shape)    \n",
    "    xAll = Concatenate(axis = 1)([x1Pool, x2Pool, x3Pool])\n",
    "    print (\"xAll.shape\", xAll.shape)\n",
    "#     xAll = Conv2D(filters=16, )\n",
    "    \n",
    "    xAll = Flatten()(xAll)\n",
    "    print (\"xAll.shape\", xAll)\n",
    "    output = Dense(1, activation= \"sigmoid\")(xAll)\n",
    "    print (\"output.shape\", output.shape)\n",
    "    modelCNN = Model(inputs = deep_inputs, outputs = output) \n",
    "    modelCNN.summary()\n",
    "    return modelCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CreateModel2D2Nets():\n",
    "    #embedding_size=32\n",
    "\n",
    "    embedding_size=50\n",
    "    deep_inputs = Input(shape=(max_words,))\n",
    "    print (\"deep_inputs.shape\", deep_inputs.shape)\n",
    "    x = Embedding(vocabulary_size, embedding_size, input_length=max_words)(deep_inputs)\n",
    "    x = Reshape((max_words,embedding_size,1) )(x)\n",
    "    print (\"x.shape\", x.shape)\n",
    "#     x = np.reshape(max_words, embedding_size, 1)(x)\n",
    "#     print (x.shape)\n",
    "    x1  = Conv2D(filters=100, kernel_size=(5, embedding_size),  activation=\"relu\")(x)\n",
    "    print (\"x1.shape\", x1.shape)\n",
    "    x1Pool = MaxPool2D (pool_size=(max_words-4, 1))(x1)    \n",
    "    print (\"x1Pool.shape\", x1Pool.shape) \n",
    "    x2 =  Conv2D(filters=100, kernel_size=(2, embedding_size),  activation=\"relu\")(x)\n",
    "    print (\"x2.shape\", x2.shape)\n",
    "    x2Pool = MaxPool2D (pool_size=(max_words-1, 1))(x2)    \n",
    "    print (\"x2Pool.shape\", x2Pool.shape)    \n",
    "    xAll = Concatenate(axis = 1)([x1Pool, x2Pool])\n",
    "    print (\"xAll.shape\", xAll.shape)\n",
    "#     xAll = Conv2D(filters=16, )\n",
    "    \n",
    "    xAll = Flatten()(xAll)\n",
    "    print (\"xAll.shape\", xAll)\n",
    "    output = Dense(1, activation= \"sigmoid\")(xAll)\n",
    "    print (\"output.shape\", output.shape)\n",
    "    modelCNN = Model(inputs = deep_inputs, outputs = output) \n",
    "    modelCNN.summary()\n",
    "    return modelCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deep_inputs.shape (?, 500)\n",
      "x.shape (?, 500, 50, 1)\n",
      "x1.shape (?, 496, 1, 100)\n",
      "x1Pool.shape (?, 1, 1, 100)\n",
      "x2.shape (?, 499, 1, 100)\n",
      "x2Pool.shape (?, 1, 1, 100)\n",
      "xAll.shape (?, 2, 1, 100)\n",
      "xAll.shape Tensor(\"flatten_1/Reshape:0\", shape=(?, ?), dtype=float32)\n",
      "output.shape (?, 1)\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 500)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 500, 50)      500000      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 500, 50, 1)   0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 496, 1, 100)  25100       reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 499, 1, 100)  10100       reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 1, 1, 100)    0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 1, 1, 100)    0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 2, 1, 100)    0           max_pooling2d_1[0][0]            \n",
      "                                                                 max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 200)          0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1)            201         flatten_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 535,401\n",
      "Trainable params: 535,401\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2NetsCNN = CreateModel2D2Nets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deep_inputs.shape (?, 500)\n",
      "x.shape (?, 500, 50, 1)\n",
      "x1.shape (?, 496, 1, 100)\n",
      "x1Pool.shape (?, 1, 1, 100)\n",
      "x2.shape (?, 499, 1, 100)\n",
      "x2Pool.shape (?, 1, 1, 100)\n",
      "x3.shape (?, 498, 1, 100)\n",
      "x3Pool.shape (?, 1, 1, 100)\n",
      "xAll.shape (?, 3, 1, 100)\n",
      "xAll.shape Tensor(\"flatten_2/Reshape:0\", shape=(?, ?), dtype=float32)\n",
      "output.shape (?, 1)\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 500)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 500, 50)      500000      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape_2 (Reshape)             (None, 500, 50, 1)   0           embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 496, 1, 100)  25100       reshape_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 499, 1, 100)  10100       reshape_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 498, 1, 100)  15100       reshape_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 1, 1, 100)    0           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)  (None, 1, 1, 100)    0           conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2D)  (None, 1, 1, 100)    0           conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 3, 1, 100)    0           max_pooling2d_3[0][0]            \n",
      "                                                                 max_pooling2d_4[0][0]            \n",
      "                                                                 max_pooling2d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 300)          0           concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1)            301         flatten_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 550,601\n",
      "Trainable params: 550,601\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model3NetsCNN = CreateModel2D3Nets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deep_inputs.shape (?, 500)\n",
      "x.shape (?, 500, 50)\n",
      "x1.shape (?, 496, 100)\n",
      "x1Pool.shape (?, 99, 100)\n",
      "x2.shape (?, 499, 100)\n",
      "x2Pool.shape (?, 249, 100)\n",
      "x3.shape (?, 498, 100)\n",
      "x3Pool.shape (?, 166, 100)\n",
      "xAll.shape (?, 514, 100)\n",
      "xAll.shape Tensor(\"flatten_3/Reshape:0\", shape=(?, ?), dtype=float32)\n",
      "output.shape (?, 1)\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 500)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 500, 50)      500000      input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape_3 (Reshape)             (None, 500, 50)      0           embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 496, 100)     25100       reshape_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 499, 100)     10100       reshape_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 498, 100)     15100       reshape_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 99, 100)      0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 249, 100)     0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 166, 100)     0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 514, 100)     0           max_pooling1d_1[0][0]            \n",
      "                                                                 max_pooling1d_2[0][0]            \n",
      "                                                                 max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 510, 150)     75150       concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)  (None, 102, 150)     0           conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 98, 200)      150200      max_pooling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1D)  (None, 19, 200)      0           conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 3800)         0           max_pooling1d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 1)            3801        flatten_3[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 779,451\n",
      "Trainable params: 779,451\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def CreateModel1D():\n",
    "    #embedding_size=32\n",
    "\n",
    "    embedding_size=50\n",
    "    deep_inputs = Input(shape=(max_words,))\n",
    "    print (\"deep_inputs.shape\", deep_inputs.shape)\n",
    "    x = Embedding(vocabulary_size, embedding_size, input_length=max_words)(deep_inputs)\n",
    "    x = Reshape((max_words,embedding_size) )(x)\n",
    "    print (\"x.shape\", x.shape)\n",
    "#     x = np.reshape(max_words, embedding_size, 1)(x)\n",
    "#     print (x.shape)\n",
    "    x1  = Conv1D(filters=100, kernel_size=5,  activation=\"relu\")(x)\n",
    "    print (\"x1.shape\", x1.shape)\n",
    "    x1Pool = MaxPool1D (pool_size=5)(x1)    \n",
    "    print (\"x1Pool.shape\", x1Pool.shape) \n",
    "    x2 =  Conv1D(filters=100, kernel_size=2,  activation=\"relu\")(x)\n",
    "    print (\"x2.shape\", x2.shape)\n",
    "    x2Pool = MaxPool1D (pool_size=2)(x2)    \n",
    "    print (\"x2Pool.shape\", x2Pool.shape)    \n",
    "    x3 =  Conv1D(filters=100, kernel_size=3,  activation=\"relu\")(x)\n",
    "    print (\"x3.shape\", x3.shape)\n",
    "    x3Pool = MaxPool1D (pool_size=3)(x3)    \n",
    "    print (\"x3Pool.shape\", x3Pool.shape)    \n",
    "    xAll = Concatenate(axis = 1)([x1Pool, x2Pool, x3Pool])\n",
    "    print (\"xAll.shape\", xAll.shape)\n",
    "#     \n",
    "    xAll = Conv1D(filters=150, kernel_size = 5, activation=\"relu\" ) (xAll)\n",
    "    xAll = MaxPool1D(pool_size=5) (xAll)\n",
    "\n",
    "    xAll = Conv1D(filters=200, kernel_size = 5, activation=\"relu\" ) (xAll)\n",
    "    xAll = MaxPool1D(pool_size=5) (xAll)\n",
    "    \n",
    "    xAll = Flatten()(xAll)\n",
    "    print (\"xAll.shape\", xAll)\n",
    "    output = Dense(1, activation= \"sigmoid\")(xAll)\n",
    "    print (\"output.shape\", output.shape)\n",
    "    modelCNN = Model(inputs = deep_inputs, outputs = output) \n",
    "    modelCNN.summary()\n",
    "    return modelCNN\n",
    "model1DCNN = CreateModel1D()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3NetsCNN.compile(loss='binary_crossentropy', \n",
    "             optimizer='adam', \n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2NetsCNN.compile(loss='binary_crossentropy', \n",
    "             optimizer='adam', \n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "num_epochs = 10\n",
    "X_valid, y_valid = X_train[:batch_size], y_train[:batch_size]\n",
    "X_train2, y_train2 = X_train[batch_size:], y_train[batch_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback_list = [#early1\n",
    "                keras.callbacks.ModelCheckpoint(filepath=\"my_CNNmod.h5\", monitor=\"val_acc\",\n",
    "                                               save_best_only=True),\n",
    "                #keras.callbacks.TerminateOnNaN()\n",
    "                ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 24744 samples, validate on 256 samples\n",
      "Epoch 1/10\n",
      "24744/24744 [==============================] - 142s 6ms/step - loss: 0.5803 - acc: 0.7072 - val_loss: 0.2982 - val_acc: 0.8789\n",
      "Epoch 2/10\n",
      "24744/24744 [==============================] - 141s 6ms/step - loss: 0.3008 - acc: 0.8763 - val_loss: 0.2298 - val_acc: 0.8984\n",
      "Epoch 3/10\n",
      "24744/24744 [==============================] - 148s 6ms/step - loss: 0.2012 - acc: 0.9252 - val_loss: 0.2254 - val_acc: 0.8906\n",
      "Epoch 4/10\n",
      "24744/24744 [==============================] - 159s 6ms/step - loss: 0.1406 - acc: 0.9522 - val_loss: 0.2209 - val_acc: 0.8984\n",
      "Epoch 5/10\n",
      "24744/24744 [==============================] - 204s 8ms/step - loss: 0.0946 - acc: 0.9719 - val_loss: 0.2459 - val_acc: 0.8984\n",
      "Epoch 6/10\n",
      "24744/24744 [==============================] - 208s 8ms/step - loss: 0.0602 - acc: 0.9851 - val_loss: 0.2703 - val_acc: 0.8906\n",
      "Epoch 7/10\n",
      "24744/24744 [==============================] - 207s 8ms/step - loss: 0.0377 - acc: 0.9931 - val_loss: 0.2863 - val_acc: 0.8945\n",
      "Epoch 8/10\n",
      "24744/24744 [==============================] - 217s 9ms/step - loss: 0.0217 - acc: 0.9973 - val_loss: 0.3354 - val_acc: 0.9023\n",
      "Epoch 9/10\n",
      "24744/24744 [==============================] - 212s 9ms/step - loss: 0.0126 - acc: 0.9990 - val_loss: 0.3639 - val_acc: 0.9062\n",
      "Epoch 10/10\n",
      "24744/24744 [==============================] - 203s 8ms/step - loss: 0.0076 - acc: 0.9998 - val_loss: 0.3992 - val_acc: 0.8984\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x26c4721eb00>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3NetsCNN.fit(X_train2, y_train2, validation_data=(X_valid, y_valid), batch_size=batch_size, \n",
    "             epochs=num_epochs, callbacks=callback_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 24744 samples, validate on 256 samples\n",
      "Epoch 1/10\n",
      "24744/24744 [==============================] - 150s 6ms/step - loss: 0.5945 - acc: 0.6908 - val_loss: 0.3346 - val_acc: 0.8594\n",
      "Epoch 2/10\n",
      "24744/24744 [==============================] - 142s 6ms/step - loss: 0.3180 - acc: 0.8678 - val_loss: 0.2215 - val_acc: 0.9023\n",
      "Epoch 3/10\n",
      "24744/24744 [==============================] - 175s 7ms/step - loss: 0.2133 - acc: 0.9176 - val_loss: 0.2054 - val_acc: 0.9141\n",
      "Epoch 4/10\n",
      "24744/24744 [==============================] - 157s 6ms/step - loss: 0.1482 - acc: 0.9486 - val_loss: 0.2110 - val_acc: 0.9141\n",
      "Epoch 5/10\n",
      "24744/24744 [==============================] - 148s 6ms/step - loss: 0.1008 - acc: 0.9693 - val_loss: 0.2207 - val_acc: 0.9023\n",
      "Epoch 6/10\n",
      "24744/24744 [==============================] - 143s 6ms/step - loss: 0.0661 - acc: 0.9835 - val_loss: 0.2333 - val_acc: 0.9102\n",
      "Epoch 7/10\n",
      "24744/24744 [==============================] - 135s 5ms/step - loss: 0.0412 - acc: 0.9925 - val_loss: 0.2484 - val_acc: 0.9180\n",
      "Epoch 8/10\n",
      "24744/24744 [==============================] - 100s 4ms/step - loss: 0.0253 - acc: 0.9970 - val_loss: 0.2586 - val_acc: 0.9258\n",
      "Epoch 9/10\n",
      "24744/24744 [==============================] - 98s 4ms/step - loss: 0.0154 - acc: 0.9985 - val_loss: 0.2838 - val_acc: 0.9062\n",
      "Epoch 10/10\n",
      "24744/24744 [==============================] - 128s 5ms/step - loss: 0.0097 - acc: 0.9995 - val_loss: 0.2816 - val_acc: 0.9062\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x26c46a89a20>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2NetsCNN.fit(X_train2, y_train2, validation_data=(X_valid, y_valid), batch_size=batch_size, \n",
    "             epochs=num_epochs, callbacks=callback_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 24744 samples, validate on 256 samples\n",
      "Epoch 1/10\n",
      "24744/24744 [==============================] - 330s 13ms/step - loss: 0.5205 - acc: 0.6949 - val_loss: 0.1966 - val_acc: 0.9258\n",
      "Epoch 2/10\n",
      "24744/24744 [==============================] - 358s 14ms/step - loss: 0.2073 - acc: 0.9198 - val_loss: 0.1640 - val_acc: 0.9258\n",
      "Epoch 3/10\n",
      "24744/24744 [==============================] - 339s 14ms/step - loss: 0.1282 - acc: 0.9535 - val_loss: 0.2043 - val_acc: 0.9102\n",
      "Epoch 4/10\n",
      "24744/24744 [==============================] - 319s 13ms/step - loss: 0.0696 - acc: 0.9764 - val_loss: 0.2330 - val_acc: 0.8945\n",
      "Epoch 5/10\n",
      "24744/24744 [==============================] - 319s 13ms/step - loss: 0.0368 - acc: 0.9882 - val_loss: 0.3412 - val_acc: 0.8984\n",
      "Epoch 6/10\n",
      "24744/24744 [==============================] - 322s 13ms/step - loss: 0.0156 - acc: 0.9952 - val_loss: 0.5031 - val_acc: 0.8906\n",
      "Epoch 7/10\n",
      "24744/24744 [==============================] - 320s 13ms/step - loss: 0.0037 - acc: 0.9992 - val_loss: 0.6392 - val_acc: 0.8906\n",
      "Epoch 8/10\n",
      "24744/24744 [==============================] - 317s 13ms/step - loss: 8.9361e-04 - acc: 0.9999 - val_loss: 0.6841 - val_acc: 0.9023\n",
      "Epoch 9/10\n",
      "24744/24744 [==============================] - 316s 13ms/step - loss: 1.7891e-04 - acc: 1.0000 - val_loss: 0.6986 - val_acc: 0.8984\n",
      "Epoch 10/10\n",
      "24744/24744 [==============================] - 320s 13ms/step - loss: 7.2827e-05 - acc: 1.0000 - val_loss: 0.7109 - val_acc: 0.8945\n",
      "deep_inputs.shape (?, 500)\n",
      "x.shape (?, 500, 50, 1)\n",
      "x1.shape (?, 496, 1, 100)\n",
      "x1Pool.shape (?, 1, 1, 100)\n",
      "x2.shape (?, 499, 1, 100)\n",
      "x2Pool.shape (?, 1, 1, 100)\n",
      "x3.shape (?, 498, 1, 100)\n",
      "x3Pool.shape (?, 1, 1, 100)\n",
      "xAll.shape (?, 3, 1, 100)\n",
      "xAll.shape Tensor(\"flatten_7/Reshape:0\", shape=(?, ?), dtype=float32)\n",
      "output.shape (?, 1)\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_10 (InputLayer)           (None, 500)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_10 (Embedding)        (None, 500, 50)      500000      input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "reshape_10 (Reshape)            (None, 500, 50, 1)   0           embedding_10[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 496, 1, 100)  25100       reshape_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 499, 1, 100)  10100       reshape_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 498, 1, 100)  15100       reshape_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling2D) (None, 1, 1, 100)    0           conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling2D) (None, 1, 1, 100)    0           conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_13 (MaxPooling2D) (None, 1, 1, 100)    0           conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 3, 1, 100)    0           max_pooling2d_11[0][0]           \n",
      "                                                                 max_pooling2d_12[0][0]           \n",
      "                                                                 max_pooling2d_13[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_7 (Flatten)             (None, 300)          0           concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 1)            301         flatten_7[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 550,601\n",
      "Trainable params: 550,601\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Test accuracy: [0.4429193976688385, 0.87272] [0.4429193976688385, 0.87272]\n"
     ]
    }
   ],
   "source": [
    "model1DCNN.compile(loss='binary_crossentropy', \n",
    "             optimizer='adam', \n",
    "             metrics=['accuracy'])\n",
    "model1DCNN.fit(X_train2, y_train2, validation_data=(X_valid, y_valid), batch_size=batch_size, \n",
    "             epochs=num_epochs, callbacks=callback_list)\n",
    "bestCNN1D = CreateModel()\n",
    "bestCNN1D.compile(loss='binary_crossentropy', \n",
    "             optimizer='adam', \n",
    "             metrics=['accuracy'])\n",
    "bestCNN1D.load_weights(\"my_CNNmod.h5\", by_name=False)\n",
    "scoresFinal1D = modelCNN.evaluate(X_test, y_test, verbose=0)\n",
    "scoresBest1D = bestCNN.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test accuracy:', scoresFinal1D, scoresBest1D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deep_inputs.shape (?, 500)\n",
      "x.shape (?, 500, 50, 1)\n",
      "x1.shape (?, 496, 1, 100)\n",
      "x1Pool.shape (?, 1, 1, 100)\n",
      "x2.shape (?, 499, 1, 100)\n",
      "x2Pool.shape (?, 1, 1, 100)\n",
      "x3.shape (?, 498, 1, 100)\n",
      "x3Pool.shape (?, 1, 1, 100)\n",
      "xAll.shape (?, 3, 1, 100)\n",
      "xAll.shape Tensor(\"flatten_2/Reshape:0\", shape=(?, ?), dtype=float32)\n",
      "output.shape (?, 1)\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 500)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 500, 50)      500000      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape_2 (Reshape)             (None, 500, 50, 1)   0           embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 496, 1, 100)  25100       reshape_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 499, 1, 100)  10100       reshape_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 498, 1, 100)  15100       reshape_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)  (None, 1, 1, 100)    0           conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2D)  (None, 1, 1, 100)    0           conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2D)  (None, 1, 1, 100)    0           conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 3, 1, 100)    0           max_pooling2d_4[0][0]            \n",
      "                                                                 max_pooling2d_5[0][0]            \n",
      "                                                                 max_pooling2d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 300)          0           concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1)            301         flatten_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 550,601\n",
      "Trainable params: 550,601\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Test accuracy: [0.4429193976688385, 0.87272] [0.4429193976688385, 0.87272]\n"
     ]
    }
   ],
   "source": [
    "bestCNN = CreateModel()\n",
    "bestCNN.compile(loss='binary_crossentropy', \n",
    "             optimizer='adam', \n",
    "             metrics=['accuracy'])\n",
    "bestCNN.load_weights(\"my_CNNmod.h5\", by_name=False)\n",
    "scoresFinal = modelCNN.evaluate(X_test, y_test, verbose=0)\n",
    "scoresBest = bestCNN.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test accuracy:', scoresFinal, scoresBest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now going to predict these reviews\n"
     ]
    }
   ],
   "source": [
    "reviewList = [\"the movie was boring\", \"the movie was not too boring\",\n",
    "              \"the movie was not too long\",\n",
    "              \"the movie was not at all bad\", \n",
    "              \"the movie was a total waste of my time\",\n",
    "              \"the food was so deliciously delicious that i felt sinfully wicked\" ,   \n",
    "              \"the food was so wicked that after eating i felt sinful\"\n",
    "             ]\n",
    "print (\"Now going to predict these reviews\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now going to predict these reviews\n"
     ]
    }
   ],
   "source": [
    "reviewList1 = [\"the movie was boring\", \"I cannot watch such a movie\",\n",
    "              \"the food was so wicked that after eating i felt sinful\",\n",
    "              \"the meal was tasty\",\n",
    "              \"the dessert was suptuous\"\n",
    "             ]\n",
    "print (\"Now going to predict these reviews\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "print (word2id[\"the\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "review= the movie was boring\n",
      "word= the id= 4\n",
      "word= movie id= 20\n",
      "word= was id= 16\n",
      "word= boring id= 357\n",
      "Prediction Probability for  \" the movie was boring \" 3 Concat CNN  =  0.005292458 Sentiment= Negative \n",
      "\n",
      "Prediction Probability for  \" the movie was boring \" 2 Concat Nets CNN  =  0.0020353186 Sentiment= Negative \n",
      "\n",
      "review= the movie was not too boring\n",
      "word= the id= 4\n",
      "word= movie id= 20\n",
      "word= was id= 16\n",
      "word= not id= 24\n",
      "word= too id= 99\n",
      "word= boring id= 357\n",
      "Prediction Probability for  \" the movie was not too boring \" 3 Concat CNN  =  0.00031824218 Sentiment= Negative \n",
      "\n",
      "Prediction Probability for  \" the movie was not too boring \" 2 Concat Nets CNN  =  4.344389e-05 Sentiment= Negative \n",
      "\n",
      "review= the movie was not too long\n",
      "word= the id= 4\n",
      "word= movie id= 20\n",
      "word= was id= 16\n",
      "word= not id= 24\n",
      "word= too id= 99\n",
      "word= long id= 196\n",
      "Prediction Probability for  \" the movie was not too long \" 3 Concat CNN  =  0.0006158856 Sentiment= Negative \n",
      "\n",
      "Prediction Probability for  \" the movie was not too long \" 2 Concat Nets CNN  =  0.002141721 Sentiment= Negative \n",
      "\n",
      "review= the movie was not at all bad\n",
      "word= the id= 4\n",
      "word= movie id= 20\n",
      "word= was id= 16\n",
      "word= not id= 24\n",
      "word= at id= 33\n",
      "word= all id= 32\n",
      "word= bad id= 78\n",
      "Prediction Probability for  \" the movie was not at all bad \" 3 Concat CNN  =  0.048484825 Sentiment= Negative \n",
      "\n",
      "Prediction Probability for  \" the movie was not at all bad \" 2 Concat Nets CNN  =  0.008905026 Sentiment= Negative \n",
      "\n",
      "review= the movie was a total waste of my time\n",
      "word= the id= 4\n",
      "word= movie id= 20\n",
      "word= was id= 16\n",
      "word= a id= 6\n",
      "word= total id= 964\n",
      "word= waste id= 437\n",
      "word= of id= 7\n",
      "word= my id= 61\n",
      "word= time id= 58\n",
      "Prediction Probability for  \" the movie was a total waste of my time \" 3 Concat CNN  =  9.515411e-11 Sentiment= Negative \n",
      "\n",
      "Prediction Probability for  \" the movie was a total waste of my time \" 2 Concat Nets CNN  =  2.0245217e-08 Sentiment= Negative \n",
      "\n",
      "review= the food was so deliciously delicious that i felt sinfully wicked\n",
      "word= the id= 4\n",
      "word= food id= 1644\n",
      "word= was id= 16\n",
      "word= so id= 38\n",
      "word= deliciously id= 6922\n",
      "word= delicious id= 6335\n",
      "word= that id= 15\n",
      "word= i id= 13\n",
      "word= felt id= 421\n",
      "sinfully Appended 2\n",
      "word= wicked id= 3799\n",
      "Prediction Probability for  \" the food was so deliciously delicious that i felt sinfully wicked \" 3 Concat CNN  =  1.0 Sentiment= Positive \n",
      "\n",
      "Prediction Probability for  \" the food was so deliciously delicious that i felt sinfully wicked \" 2 Concat Nets CNN  =  0.9999999 Sentiment= Positive \n",
      "\n",
      "review= the food was so wicked that after eating i felt sinful\n",
      "word= the id= 4\n",
      "word= food id= 1644\n",
      "word= was id= 16\n",
      "word= so id= 38\n",
      "word= wicked id= 3799\n",
      "word= that id= 15\n",
      "word= after id= 103\n",
      "word= eating id= 1889\n",
      "word= i id= 13\n",
      "word= felt id= 421\n",
      "got a word outside teh vocab_index sinful 17010 breaking\n",
      "Prediction Probability for  \" the food was so wicked that after eating i felt sinful \" 3 Concat CNN  =  0.9732852 Sentiment= Positive \n",
      "\n",
      "Prediction Probability for  \" the food was so wicked that after eating i felt sinful \" 2 Concat Nets CNN  =  0.49287388 Sentiment= Negative \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def PredictSentiment(reviewList, ModelListTuple):\n",
    "    sentiment= {True: \"Positive\",\n",
    "               False: \"Negative\"}\n",
    "    Threshold = 0.5\n",
    "    for r in reviewList:\n",
    "        words = r.split()\n",
    "        review = []\n",
    "        print (\"review=\", r)\n",
    "        for word in words:\n",
    "          if word not in word2id: \n",
    "            review.append(2)\n",
    "            print (word, \"Appended 2\")\n",
    "          else:\n",
    "            if (word2id[word]) >= vocabulary_size:\n",
    "                print(\"got a word outside teh vocab_index\", word, word2id[word], \"breaking\")\n",
    "                break\n",
    "            print (\"word=\", word, \"id=\", word2id[word])\n",
    "            review.append(word2id[word]) \n",
    "        review = keras.preprocessing.sequence.pad_sequences([review],\n",
    "          truncating='pre', padding='pre', maxlen=max_words)\n",
    "        for i,m in enumerate(ModelListTuple):\n",
    "            if m[0] is not None:\n",
    "                prediction = m[0].predict(review)\n",
    "                print(\"Prediction Probability for \", \"\\\"\",r, \"\\\"\",ModelListTuple[i][1],\" = \", prediction[0][0], \"Sentiment=\", \n",
    "                      sentiment[prediction[0][0]>Threshold], \"\\n\")\n",
    "                \n",
    "PredictSentiment(reviewList,[(model3NetsCNN, \"3 Concat CNN\"), (model2NetsCNN, \"2 Concat Nets CNN\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "review= the movie was boring\n",
      "word= the id= 4\n",
      "word= movie id= 20\n",
      "word= was id= 16\n",
      "word= boring id= 357\n",
      "Prediction Probability for  \" the movie was boring \" 3 Concat CNN  =  0.005292458 Sentiment= Negative \n",
      "\n",
      "Prediction Probability for  \" the movie was boring \" 2 Concat Nets CNN  =  0.0020353186 Sentiment= Negative \n",
      "\n",
      "review= I cannot watch such a movie\n",
      "I Appended 2\n",
      "word= cannot id= 566\n",
      "word= watch id= 106\n",
      "word= such id= 141\n",
      "word= a id= 6\n",
      "word= movie id= 20\n",
      "Prediction Probability for  \" I cannot watch such a movie \" 3 Concat CNN  =  0.2745939 Sentiment= Negative \n",
      "\n",
      "Prediction Probability for  \" I cannot watch such a movie \" 2 Concat Nets CNN  =  0.06950959 Sentiment= Negative \n",
      "\n",
      "review= the food was so wicked that after eating i felt sinful\n",
      "word= the id= 4\n",
      "word= food id= 1644\n",
      "word= was id= 16\n",
      "word= so id= 38\n",
      "word= wicked id= 3799\n",
      "word= that id= 15\n",
      "word= after id= 103\n",
      "word= eating id= 1889\n",
      "word= i id= 13\n",
      "word= felt id= 421\n",
      "got a word outside teh vocab_index sinful 17010 breaking\n",
      "Prediction Probability for  \" the food was so wicked that after eating i felt sinful \" 3 Concat CNN  =  0.9732852 Sentiment= Positive \n",
      "\n",
      "Prediction Probability for  \" the food was so wicked that after eating i felt sinful \" 2 Concat Nets CNN  =  0.49287388 Sentiment= Negative \n",
      "\n",
      "review= the meal was tasty\n",
      "word= the id= 4\n",
      "word= meal id= 6979\n",
      "word= was id= 16\n",
      "word= tasty id= 8540\n",
      "Prediction Probability for  \" the meal was tasty \" 3 Concat CNN  =  0.9984843 Sentiment= Positive \n",
      "\n",
      "Prediction Probability for  \" the meal was tasty \" 2 Concat Nets CNN  =  0.42028597 Sentiment= Negative \n",
      "\n",
      "review= the dessert was suptuous\n",
      "word= the id= 4\n",
      "got a word outside teh vocab_index dessert 20151 breaking\n",
      "Prediction Probability for  \" the dessert was suptuous \" 3 Concat CNN  =  0.48466977 Sentiment= Negative \n",
      "\n",
      "Prediction Probability for  \" the dessert was suptuous \" 2 Concat Nets CNN  =  0.48478568 Sentiment= Negative \n",
      "\n"
     ]
    }
   ],
   "source": [
    "PredictSentiment(reviewList1,[(model3NetsCNN, \"3 Concat CNN\"), (model2NetsCNN, \"2 Concat Nets CNN\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_similarity(vec_u, vec_v):\n",
    "    import numpy as np\n",
    "    distance = 0.0   \n",
    "    dot = np.inner(vec_u,vec_v)\n",
    "    norm_vec_u = np.linalg.norm(vec_u)\n",
    "    norm_vec_v = np.linalg.norm(vec_v)\n",
    "    cos_similarity_val = dot/(norm_vec_u*norm_vec_v)\n",
    "    \n",
    "    return cos_similarity_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Layers =  17\n",
      "<class 'keras.engine.input_layer.InputLayer'>\n",
      "0\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-63-7a4e15ea3af3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0ml\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mword2id\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"apple\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword2id\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"orange\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"word of 6335\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mid2word\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m6335\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"word of 6922\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mid2word\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m6922\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"apple\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword2id\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"apple\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "model = modelCNN\n",
    "layer_outputs = [layer.output for layer in model.layers]\n",
    "i = 0\n",
    "print (\"#Layers = \", len(layer_outputs))\n",
    "print (type(model.layers[0]))\n",
    "l = model.layers[0]\n",
    "print (len(l.get_weights()))\n",
    "print (l.get_weights()[0].shape)\n",
    "print (word2id[\"apple\"], word2id[\"orange\"], \"word of 6335\", id2word[6335], \"word of 6922\", id2word[6922])\n",
    "print (\"apple\", l.get_weights()[0][word2id[\"apple\"]])\n",
    "print (\"orange\", l.get_weights()[0][word2id[\"orange\"]])\n",
    "print (\"delicious\", l.get_weights()[0][6335])\n",
    "print (\"deliciously\", l.get_weights()[0][6922])\n",
    "print (\"Similarity between delicious and deliciously\", cos_similarity(l.get_weights()[0][6335], l.get_weights()[0][6922]))\n",
    "print (\"Similarity between apple and orange\", cos_similarity(l.get_weights()[0][word2id[\"apple\"]], \n",
    "                                                             l.get_weights()[0][word2id[\"orange\"]]))\n",
    "print (\"Similarity between apple and delicious\", cos_similarity(l.get_weights()[0][word2id[\"apple\"]], \n",
    "                                                             l.get_weights()[0][6335]))\n",
    "print (\"Similarity between tasty and delicious\", cos_similarity(l.get_weights()[0][word2id[\"tasty\"]], \n",
    "                                                             l.get_weights()[0][word2id[\"delicious\"]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now use patience, and early stopping\n",
    "#early1 = keras.callbacks.EarlyStopping(monitor =\"val_acc\", patience=1, restore_best_weights=True)\n",
    "#early1 = keras.callbacks.EarlyStopping(monitor =\"acc\", patience=2, baseline = 0.95)\n",
    "#early1 = keras.callbacks.EarlyStopping(monitor =\"val_acc\", patience=1, restore_best_weights=True)\n",
    "\n",
    "callback_list = [#early1\n",
    "                keras.callbacks.ModelCheckpoint(filepath=\"my_mod2.h5\", monitor=\"val_acc\",\n",
    "                                               save_best_only=True),\n",
    "                #keras.callbacks.TerminateOnNaN()\n",
    "                ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', \n",
    "             optimizer='adam', \n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train3 = X_train2[:1000]\n",
    "Y_train3 = y_train2[:1000]\n",
    "print (X_train3.shape, Y_train3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train3, Y_train3, validation_data=(X_valid, y_valid), batch_size=batch_size, epochs=15, #num_epochs,\n",
    "             callbacks=callback_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PredictSentiment(reviewList,[(modelRNN, \"RNN\"), (model, \"CNN Checkpoint\"), (modelLSTM, \"LSTM\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
