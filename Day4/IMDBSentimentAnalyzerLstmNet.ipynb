{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This demonstrates the following:\n",
    "How to load imdb data\n",
    "How load converts words into integer indexes\n",
    "How words are stored in highest occuring freq to lowest occurring\n",
    "How to take top 5000 or 10000 vocbualry size\n",
    "idiosyncracy of imdb.get_word_index() \n",
    "Re-convert the index to take into account padding, start, out-of-vocabulary\n",
    "What is an embedding layer\n",
    "Create a ConvNet model\n",
    "Create a LSTM model\n",
    "DO the sentiment analyzer\n",
    "Predict sample sentences\n",
    "Then this takes into advanced featues of Keras such as callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.datasets import imdb\n",
    "from keras.layers import Dense,  Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMDB dataset  25000 training samples, 25000 test samples\n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = 20000\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words = vocabulary_size)\n",
    "print('IMDB dataset  {} training samples, {} test samples'.format(len(X_train), len(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Features: review in number sequence After padding---\n",
      "[1, 6740, 365, 1234, 5, 1156, 354, 11, 14, 5327, 6638, 7, 1016, 10626, 5940, 356, 44, 4, 1349, 500, 746, 5, 200, 4, 4132, 11, 16393, 9363, 1117, 1831, 7485, 5, 4831, 26, 6, 2, 4183, 17, 369, 37, 215, 1345, 143, 2, 5, 1838, 8, 1974, 15, 36, 119, 257, 85, 52, 486, 9, 6, 2, 8564, 63, 271, 6, 196, 96, 949, 4121, 4, 2, 7, 4, 2212, 2436, 819, 63, 47, 77, 7175, 180, 6, 227, 11, 94, 2494, 2, 13, 423, 4, 168, 7, 4, 22, 5, 89, 665, 71, 270, 56, 5, 13, 197, 12, 161, 5390, 99, 76, 23, 2, 7, 419, 665, 40, 91, 85, 108, 7, 4, 2084, 5, 4773, 81, 55, 52, 1901]\n",
      "Sentiment 1 = positve, 0 = negative\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print('---Features: review in number sequence After padding---')\n",
    "print(X_train[6])\n",
    "print('Sentiment 1 = positve, 0 = negative')\n",
    "print(y_train[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88584 the 1\n"
     ]
    }
   ],
   "source": [
    "w2id = imdb.get_word_index() \n",
    "id2word = {i: word  for word, i in w2id.items()}\n",
    "print (len(w2id), id2word[1], w2id[\"the\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the boiled full involving to impressive boring this as murdering naschy br villain council suggestion need has of costumes b message to may of props this echoed concentrates concept issue skeptical to god's he is and unfolds movie women like isn't surely i'm and to toward in here's for from did having because very quality it is and starship really book is both too worked carl of and br of reviewer closer figure really there will originals things is far this make mistakes and was couldn't of few br of you to don't female than place she to was between that nothing dose movies get are and br yes female just its because many br of overly to descent people time very bland \n",
      "And sentiment is  1\n"
     ]
    }
   ],
   "source": [
    "#Jibberish data?\n",
    "actual_data = []\n",
    "for i in range (len(X_train[6])):\n",
    "    ind = X_train[6][i]\n",
    "    if(ind != 0):\n",
    "        actual_data.append(id2word[ind])\n",
    "actual_data = \" \".join (actual_data )\n",
    "print (actual_data, \"\\nAnd sentiment is \", y_train[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'> 88587\n",
      "__START__ __UNK__ the and a voorhees'\n",
      "__START__ lavish production values and solid performances in this straightforward adaption of jane austen's satirical classic about the marriage game within and between the classes in provincial 18th century england northam and paltrow are a __UNK__ mixture as friends who must pass through __UNK__ and lies to discover that they love each other good humor is a __UNK__ virtue which goes a long way towards explaining the __UNK__ of the aged source material which has been toned down a bit in its harsh __UNK__ i liked the look of the film and how shots were set up and i thought it didn't rely too much on __UNK__ of head shots like most other films of the 80s and 90s do very good results \n",
      "And sentiment is  1\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "word2id ={w: i+3 for w, i in w2id.items()}\n",
    "word2id[\"__PADDING__\"] = 0\n",
    "word2id[\"__START__\"] = 1\n",
    "word2id[\"__UNK__\"] = 2\n",
    "\n",
    "#This returns the index of the words from 1 to n with 1 being the most frequently occuring word, \n",
    "\n",
    "# and n the least frequently occuring word\n",
    "\n",
    "print (type(word2id), len (word2id))\n",
    "\n",
    "id2word = {i: word  for word, i in word2id.items()}\n",
    "print(id2word[1], id2word[2], #id2word[3],#\n",
    "      id2word[4], id2word[5], id2word[6], id2word[88586])\n",
    "actual_data = []\n",
    "for i in range (len(X_train[6])):\n",
    "    ind = X_train[6][i]\n",
    "    if(ind != 0):\n",
    "        actual_data.append(id2word[ind])\n",
    "actual_data = \" \".join (actual_data )\n",
    "print (actual_data, \"\\nAnd sentiment is \", y_train[6])\n",
    "print (word2id[\"the\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum review length: 2697\n"
     ]
    }
   ],
   "source": [
    "print('Maximum review length: {}'.format(\n",
    "len((max((X_train + X_test), key=len)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "max_words = 500\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_words)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "#embedding_size=32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "num_epochs = 3\n",
    "X_valid, y_valid = X_train[:batch_size], y_train[:batch_size]\n",
    "X_train2, y_train2 = X_train[batch_size:], y_train[batch_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 500, 100)          2000000   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 150)               150600    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 151       \n",
      "=================================================================\n",
      "Total params: 2,150,751\n",
      "Trainable params: 2,150,751\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#LEts now create an LSTM model\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "\n",
    "embedding_size=100\n",
    "modelLSTM=Sequential()\n",
    "modelLSTM.add(Embedding(vocabulary_size, embedding_size, input_length=max_words))\n",
    "modelLSTM.add(LSTM(150, dropout=0.2)) \n",
    "modelLSTM.add(Dense(1, activation='sigmoid'))\n",
    "print(modelLSTM.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelLSTM.compile(loss='binary_crossentropy', \n",
    "             optimizer='adam', \n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "390/390 [==============================] - 283s 721ms/step - loss: 0.4220 - accuracy: 0.8076 - val_loss: 0.2517 - val_accuracy: 0.9062\n",
      "Epoch 2/3\n",
      "390/390 [==============================] - 314s 805ms/step - loss: 0.2300 - accuracy: 0.9127 - val_loss: 0.2375 - val_accuracy: 0.9062\n",
      "Epoch 3/3\n",
      "390/390 [==============================] - 279s 715ms/step - loss: 0.1876 - accuracy: 0.9297 - val_loss: 0.3043 - val_accuracy: 0.8906\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1d1e91554c0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelLSTM.fit(X_train2, y_train2, validation_data=(X_valid, y_valid), batch_size=batch_size, epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.8479999899864197\n"
     ]
    }
   ],
   "source": [
    "scores = modelLSTM.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now going to predict these reviews\n"
     ]
    }
   ],
   "source": [
    "reviewList = [\"the movie was not so bad\", \n",
    "              \"the movie was a total waste of my time\",\n",
    "              \"the food was so deliciously delicious that i felt sinfully wicked\"                 \n",
    "             ]\n",
    "print (\"Now going to predict these reviews\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print (w2id[\"the\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "review= the movie was not so bad\n",
      "word= the id= 4\n",
      "word= movie id= 20\n",
      "word= was id= 16\n",
      "word= not id= 24\n",
      "word= so id= 38\n",
      "word= bad id= 78\n",
      "LSTM Prediction Probability =  0.21577564 Sentiment= Negative \n",
      "\n",
      "review= the movie was a total waste of my time\n",
      "word= the id= 4\n",
      "word= movie id= 20\n",
      "word= was id= 16\n",
      "word= a id= 6\n",
      "word= total id= 964\n",
      "word= waste id= 437\n",
      "word= of id= 7\n",
      "word= my id= 61\n",
      "word= time id= 58\n",
      "LSTM Prediction Probability =  0.039051354 Sentiment= Negative \n",
      "\n",
      "review= the food was so deliciously delicious that i felt sinfully wicked\n",
      "word= the id= 4\n",
      "word= food id= 1644\n",
      "word= was id= 16\n",
      "word= so id= 38\n",
      "word= deliciously id= 6922\n",
      "word= delicious id= 6335\n",
      "word= that id= 15\n",
      "word= i id= 13\n",
      "word= felt id= 421\n",
      "sinfully Appended 2\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'sinfully'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-300af4f19460>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"got a word outside teh vocab_index\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword2id\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"breaking\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"word=\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"id=\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword2id\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m         \u001b[0mreview\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword2id\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     review = keras.preprocessing.sequence.pad_sequences([review],\n",
      "\u001b[1;31mKeyError\u001b[0m: 'sinfully'"
     ]
    }
   ],
   "source": [
    "sentiment= {True: \"Positive\",\n",
    "           False: \"Negative\"}\n",
    "Threshold = 0.5\n",
    "for r in reviewList:\n",
    "    words = r.split()\n",
    "    review = []\n",
    "    print (\"review=\", r)\n",
    "    for word in words:\n",
    "        \n",
    "        if word not in word2id: \n",
    "            review.append(2)\n",
    "            print (word, \"Appended 2\")\n",
    "        else:\n",
    "            if (word2id[word]) >= vocabulary_size:\n",
    "                print(\"got a word outside teh vocab_index\", word, word2id[word], \"breaking\")\n",
    "                break\n",
    "        print (\"word=\", word, \"id=\", word2id[word])\n",
    "        review.append(word2id[word]) \n",
    "    review = keras.preprocessing.sequence.pad_sequences([review],\n",
    "      truncating='pre', padding='pre', maxlen=max_words)\n",
    "    #prediction = model.predict(review)\n",
    "    #print(\"Conv Prediction Probability = \", prediction[0][0], \"Sentiment=\", \n",
    "          #sentiment[prediction[0][0]>Threshold])\n",
    "\n",
    "    predictionLSTM = modelLSTM.predict(review)\n",
    "    print(\"LSTM Prediction Probability = \", predictionLSTM[0][0], \"Sentiment=\", \n",
    "      sentiment[predictionLSTM[0][0]>Threshold],\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Layers =  10\n",
      "<class 'keras.layers.embeddings.Embedding'>\n",
      "1\n",
      "(10000, 50)\n",
      "7695 4999 delicious deliciously\n",
      "[ 0.04624722  0.07545268  0.02489838 -0.02055413  0.04391449 -0.04921789\n",
      " -0.05327091  0.07488748  0.07766633 -0.08686695 -0.04349653 -0.012655\n",
      "  0.0308004  -0.01143026  0.00359003  0.01599179 -0.02458988  0.04710847\n",
      "  0.02592155 -0.0724646   0.03693506  0.04010747 -0.06797239 -0.07562222\n",
      " -0.06412911  0.05386121 -0.06815677  0.04249343 -0.07385083  0.00414237\n",
      " -0.00822238 -0.06516083 -0.05180028  0.01603695 -0.08868137 -0.01046827\n",
      "  0.00348281  0.05581423 -0.07998845  0.02918507  0.0428033   0.05758673\n",
      "  0.0043446  -0.03834175  0.02955479  0.02539401 -0.0298327   0.00650966\n",
      "  0.06053158 -0.08526278]\n",
      "[-0.02442165  0.07331695  0.06798726 -0.02431521  0.05838734 -0.03160905\n",
      " -0.00268014  0.0575944   0.06967107 -0.0380679  -0.0452188   0.07590131\n",
      "  0.02583498  0.06979147 -0.00759339 -0.00991021 -0.06585842  0.01826239\n",
      "  0.04959083  0.02211002  0.07902415  0.05557435 -0.00121781 -0.01968964\n",
      " -0.07954403  0.0763062  -0.04681044  0.01045032 -0.05224477 -0.05197021\n",
      " -0.0794743  -0.05599922 -0.04213225  0.00801489 -0.08030282 -0.08518251\n",
      " -0.07386032  0.02685029 -0.03687633  0.06822097  0.08527547  0.03687117\n",
      " -0.04209682 -0.07893533  0.0709362   0.06428137 -0.00412142 -0.05177616\n",
      "  0.08032116 -0.07999656]\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "layer_outputs = [layer.output for layer in model.layers]\n",
    "i = 0\n",
    "print (\"#Layers = \", len(layer_outputs))\n",
    "print (type(model.layers[0]))\n",
    "l = model.layers[0]\n",
    "print (len(l.get_weights()))\n",
    "print (l.get_weights()[0].shape)\n",
    "print (word2id[\"apple\"], word2id[\"orange\"], id2word[6335], id2word[6922])\n",
    "print (l.get_weights()[0][word2id[\"apple\"]])\n",
    "print (l.get_weights()[0][word2id[\"orange\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now use patience, and early stopping\n",
    "#early1 = keras.callbacks.EarlyStopping(monitor =\"val_acc\", patience=1, restore_best_weights=True)\n",
    "#early1 = keras.callbacks.EarlyStopping(monitor =\"val_acc\", patience=1, baseline = 0.95)\n",
    "early1 = keras.callbacks.EarlyStopping(monitor =\"acc\", patience=1, restore_best_weights=True)\n",
    "\n",
    "callback_list = [early1,\n",
    "                keras.callbacks.ModelCheckpoint(filepath=\"my_mod1.h5\", monitor=\"val_loss\",\n",
    "                                               save_best_only=True),\n",
    "                keras.callbacks.TerminateOnNaN()\n",
    "                ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', \n",
    "             optimizer='adam', \n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 500) (1000,)\n"
     ]
    }
   ],
   "source": [
    "X_train3 = X_train2[:1000]\n",
    "Y_train3 = y_train2[:1000]\n",
    "print (X_train3.shape, Y_train3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1000 samples, validate on 256 samples\n",
      "Epoch 1/15\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.0149 - acc: 0.9950 - val_loss: 0.8260 - val_acc: 0.8672\n",
      "Epoch 2/15\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 5.3326e-04 - acc: 1.0000 - val_loss: 0.4994 - val_acc: 0.9062\n",
      "Epoch 3/15\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 1.1915e-04 - acc: 1.0000 - val_loss: 0.5057 - val_acc: 0.9180\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x247913d5320>"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train3, Y_train3, validation_data=(X_valid, y_valid), batch_size=batch_size, epochs=15, #num_epochs,\n",
    "             callbacks=callback_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
